{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Information that is missing or hard to find in the Cray PE manual or LUMI docs","text":"<p>This unofficial site contains things I discovered in the HPE-Cray PE or other software on LUMI but have trouble finding in the documentation or is too specialised or anecdotal to have a place in the documentation.</p> <ol> <li>LMOD<ol> <li>Lmod setup</li> <li>Custom paths</li> </ol> </li> <li>Compilers<ol> <li>Miscellaneous topics</li> </ol> </li> <li>Slurm<ol> <li>Doc links</li> <li>Miscellaneous tips&amp;tricks</li> </ol> </li> <li>AI software</li> </ol>"},{"location":"0_00_introduction/","title":"Introduction","text":"<p>This unofficial site contains things I discovered in the HPE-Cray PE but have trouble finding in the documentation (probably also because the documentation is fairly hard to find on the web).</p>"},{"location":"1_LMOD/","title":"LMOD","text":"<ul> <li>Lmod setup</li> <li>Custom paths</li> </ul>"},{"location":"1_LMOD/1_01_LMOD_setup/","title":"Cray Lmod setup","text":"<p>TODO</p>"},{"location":"1_LMOD/1_01_LMOD_setup/#problems","title":"Problems","text":"<ul> <li> <p><code>module spider</code> does not work correctly.</p> </li> <li> <p>Noise about problems with the cache. We do note occasional cahce problems on LUMI     but those don't seem to be associated with specific properties of the HPE Cray PE     Lmod setup but rather with the system not detecting that modules have been added     to the tree. I guess Linux with Lustre may have no notification system to detect     the moment of change in a subdirectory to see if a cache file is still valid and     to ignore (system cahce) or regenerate (user cache) it otherwise.</p> </li> </ul>"},{"location":"1_LMOD/1_02_custom_paths/","title":"Custom paths","text":"<p>The information on this page is partly the result of studying <code>/opt/cray/pe/lmod_scripts/default/scripts/lmodHierarchy.lua</code> and other  module code.</p> <p>The HPE Cray PE can load custom paths with system-installed or user-installed  modules in a way that fits in the hierarchy defined by the PE. The following options are available:</p> <ul> <li> <p>Modules that depend on the CPU target module only.</p> <p>In the PE, these are in the <code>cpu</code> subdirectory and used for the <code>cray-fftw</code> modules.</p> <p>As these modules require loading only one module to be made available, there is no <code>handshake_</code> routine involved in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the network target module only, so not on the compiler or the     CPU architecture.</p> <p>In the PE, these are in the <code>net</code> subdirectory and used for the <code>cray-openshmemx</code> modules.</p> <p>As these modules require loading only one module to be made available, there is no <code>handshake_</code> routine involved in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the compiler only (so for a generic CPU)</p> <p>In the PE, these are in the <code>compiler</code> subdirectory and used for the <code>cray-hdf5</code>  modules.</p> <p>As these modules require loading only one module to be made available, there is no <code>handshake_</code> routine involved in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the CPU target module and the compiler.</p> <p>There are currently no such examples in the PE. however, the code is prepared for  future addition of such directories in the PE itself also and the reserved directory name is <code>comcpu</code>. </p> <p>As these modules require loading of two modules to be made available which in the PE can happen in any order as the hierarchy is not build in the typical Lmod way,  adding of both the PE and custom paths to <code>MODULEPATH</code> is managed by the <code>lmodHierarchy.handshake_comcpu</code> routine in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the network target module and the compiler.</p> <p>In the PE, these are in the <code>comnet</code> subdirectory and used for the <code>cray-mpich</code>, <code>cray-mpich-abi</code>, 'cray-mpich-ucx<code>and</code>cray-mpich-ucx-abi` modules.</p> <p>As these modules require loading of two modules to be made available which in the PE can happen in any order as the hierarchy is not build in the typical Lmod way,  adding of both the PE and custom paths to <code>MODULEPATH</code> is managed by the <code>lmodHierarchy.handshake_comnet</code> routine in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the network target module, the compiler and the MPI library.</p> <p>In the Cray PE, these are in the <code>mpi</code> subdirectory and used for the <code>cray-hdf5-parallel</code> and <code>cray-parallel-netcdf</code> modules.</p> <p>The addicion of the <code>mpi</code> subdiredctories is done in a way that is different from the other ones that depend on multiple modules, likely because the <code>cray-mpich-*</code> modules that add this level to the hierarchy themselves are at the <code>comnet</code> level and changed as soon as the network target or compiler module is changed, so that the regular Lmod approach of building a hierarchy can be used. The code that adds both the PE <code>mpi</code> directory and the corresponding custom path to the <code>MODULEPATH</code> is largely contained in  the <code>cray-mpich-*</code> modules but does use the <code>lmodHierarchy.get_user_custom_path</code> routine from <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on both the network and CPU target modules, on the compiler and      on the MPI library.</p> <p>There are currently no such examples in the PE. however, the code is prepared for  future addition of such directories in the PE itself also and the reserved directory name is <code>cncm</code>. </p> <p>As these modules require loading of two modules to be made available which in the PE can happen in any order as the hierarchy is not build in the typical Lmod way,  adding of both the PE and custom paths to <code>MODULEPATH</code> is managed by the <code>lmodHierarchy.handshake_cncm</code> routine in <code>lmodHierarchy.lua</code>.</p> </li> </ul> <pre><code>module_root\n\u251c\u2500 cpu\n\u2502  \u251c\u2500 x86-64\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u251c\u2500 x86-milan\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u251c\u2500 x86-rome\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u2514\u2500 x86-trento\n\u2502     \u2514\u2500 1.0\n\u251c\u2500 net\n\u2502  \u251c\u2500 ofi\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u2514\u2500 ucx\n\u2502     \u2514\u2500 1.0\n\u251c\u2500 compiler\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u251c\u2500 comcpu\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2502     \u251c\u2500 x86-64\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u251c\u2500 x86-milan\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u251c\u2500 x86-rome\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u2514\u2500 x86-trento\n\u2502  \u2502        \u2514\u2500 1.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u251c\u2500 comnet\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2502     \u251c\u2500 ofi\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u2514\u2500 ucx\n\u2502  \u2502        \u2514\u2500 1.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u251c\u2500 mpi\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2502     \u251c\u2500 ofi\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u2502     \u2514\u2500 cray-mpich\n\u2502  \u2502     \u2502        \u2514\u2500 8.0\n\u2502  \u2502     \u2514\u2500 ucx\n\u2502  \u2502        \u2514\u2500 1.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u2514\u2500 cncm\n   \u251c\u2500 aocc\n   \u2502  \u251c\u2500 2.2\n   \u2502  \u2514\u2500 3.0\n   \u251c\u2500 crayclang\n   \u2502  \u2514\u2500 10.0\n   \u2502     \u251c\u2500 ofi\n   \u2502     \u2502  \u2514\u2500 1.0\n   \u2502     \u2502     \u251c\u2500 x86-64\n   \u2502     \u2502     \u2502  \u2514\u2500 1.0\n   \u2502     \u2502     \u2502     \u2514\u2500 cray-mpich\n   \u2502     \u2502     \u2502        \u2514\u2500 8.0\n   \u22ee     \u22ee      \u22ee   \n   \u2502     \u2514\u2500 ucx\n   \u2502        \u2514\u2500 1.0\n   \u2514\u2500 gnu\n      \u2514\u2500 8.0\n</code></pre> <p>Given that the custom paths are not set by a single environment variable pointing to the roots of the custom installation directories and imposing the same directory structure as used by the PE, but instead by separate environment variables for every leaf in the picture below, the number of environment variables can explode.</p> <p>On LUMI, as of July 2022, we have:</p> <ul> <li>3 CPU target modules and the generic one if we want to support that also,     so 4 CPU target modules.</li> <li>1 network library (though one could argue that UCX can still be used on the      login, large memory and visualisation GPU nodes but these are not really meant     for heavy MPI work unless they are integrating with jobs on the regular compute      nodes)</li> <li>4 compiler ABIs, with one to be dropped soon (aocc/2.2, aocc/3.0, crayclang/10.0 and gnu/8.0)</li> <li>1 MPI ABI version (cray-mpich/8.0)</li> </ul> <p>This may result in:</p> <ul> <li>4 <code>LMOD_CUSTOM_CPU_*</code> environment variables</li> <li>1 <code>LMOD_CUSTOM_NET_*</code> environment variable (or 2 with UCX)</li> <li>4 <code>LMOD_CUSTOM_COMPILER_*</code> environment variables</li> <li>16 <code>LMOD_CUSTOM_COMCPU_*</code> environment variables</li> <li>4 <code>LMOD_CUSTOM_COMNET_*</code> environment variables (8 with UCX)</li> <li>4 <code>LMOD_CUSTOM_MPI_*</code> environment variables (8 with UCX)</li> <li>16 <code>LMOD_CUSTOM_CNCM_*</code> environment variables (24 with UCX as two CPU architectures don't make sense with UCX     on LUMI)</li> </ul> <p>The names of the environment variables are derived from the above directory structure, but</p> <ul> <li>All characters uppercase</li> <li><code>/</code>, <code>-</code>and <code>.</code> get substituted by an underscore character</li> <li>The resulting name is prefixed with <code>LMOD_CUSTOM_</code> and postfixed with <code>_PREFIX</code>.</li> </ul>"},{"location":"2_Compilers/","title":"Compilers","text":"<ul> <li>Miscellaneous topics</li> </ul>"},{"location":"2_Compilers/2_10_misc/","title":"Miscellaneous topics","text":""},{"location":"2_Compilers/2_10_misc/#using-the-compilers-without-the-wrappers","title":"Using the compilers without the wrappers","text":"Compiler module C compiler C++ compiler Fortran fixed format Fortran free format cce <code>craycc</code>, <code>clang</code> <code>crayCC</code>, <code>craycxx</code>, <code>clang++</code> <code>crayftn</code> <code>crayftn</code> gcc <code>gcc</code> <code>g++</code> <code>gfortran</code> <code>gfortran</code> aocc <code>clang</code> <code>clang++</code> <code>flang</code> <code>flang</code> rocm <code>amdclang</code> <code>amdclang++</code> <code>amdflang</code> <code>amdflang</code> <p>Remarks</p> <ul> <li> <p>The Cray clang compilers need to be pointed to the version of gcc to use using the     <code>--gcc-toolchain=&lt;value&gt;</code> option. The wrapper adds something like     <code>--gcc-toolchain=/opt/cray/pe/gcc/8.1.0/snos</code>.     Otherwise the compile will fail to find a suitable set of include files.</p> <p>TODO: Test this.</p> </li> <li> <p>A good trick to figure out which options the PE wrappers add is to use the     <code>-craype-verbose</code> flag on the command line of the wrappers.</p> </li> </ul>"},{"location":"3_Slurm/","title":"Slurm on LUMI","text":""},{"location":"3_Slurm/3_01_Doclinks/","title":"Slurm documentation links","text":"<p>Note: Check the version of Slurm after each LUMI update.</p> <p>The version after the October-November 2023 update is 22.05.10.</p>"},{"location":"3_Slurm/3_01_Doclinks/#web-documentation","title":"Web documentation","text":"<ul> <li> <p>Slurm version 22.05.10, on the system at the time of the course</p> </li> <li> <p>Slurm manual pages are also all on the web      and are easily found by Google, but are usually those for the latest version.</p> <ul> <li> <p><code>man sbatch</code></p> </li> <li> <p><code>man srun</code></p> </li> <li> <p><code>man salloc</code></p> </li> <li> <p><code>man squeue</code></p> </li> <li> <p><code>man scancel</code></p> </li> <li> <p><code>man sinfo</code></p> </li> <li> <p><code>man sstat</code></p> </li> <li> <p><code>man sacct</code></p> </li> <li> <p><code>man scontrol</code></p> </li> </ul> </li> </ul>"},{"location":"3_Slurm/3_10_misc/","title":"Miscellaneous Slurm tips&amp;trics","text":""},{"location":"3_Slurm/3_10_misc/#system-configuration","title":"System configuration","text":"<ul> <li> <p>Extensive information about the partitions:     <pre><code>scontrol show partition --all\n</code></pre>     With `--all`` to also see hidden partitions.</p> </li> <li> <p>Show topology:     <pre><code>scontrol show topology\n</code></pre></p> </li> <li> <p>Show extensive configuration info:     <pre><code>scontrol show config\n</code></pre></p> </li> <li> <p>Also interesting:     <pre><code>scontrol show assoc_mgr\n</code></pre></p> </li> </ul>"},{"location":"3_Slurm/3_10_misc/#user-or-project-related","title":"User- or project-related","text":"<ul> <li> <p>Show which partitions a project has access to:     <pre><code>sacctmgr show assoc where account=project_462000087\n</code></pre></p> </li> <li> <p>Show which partitions a user has access to:     <pre><code>sacctmgr show assoc format=account,Partition  -p Users=kurtlust\n</code></pre></p> </li> </ul>"},{"location":"3_Slurm/3_10_misc/#job-management","title":"Job management","text":"<ul> <li>History of slurm jobs:<ul> <li>Through <code>sacct</code> <pre><code>sacct -a\n</code></pre></li> <li>Through <code>slurm</code> <pre><code>slurm history 2hours -a \n</code></pre>     or from a particular user:     <pre><code>slurm history 1day -u &lt;username&gt;\n</code></pre></li> </ul> </li> </ul>"},{"location":"3_Slurm/3_10_misc/#misc","title":"Misc","text":"<ul> <li> <p>Attach to a running job rather than ssh into a node:     <pre><code>srun --pty --jobid &lt;jobid&gt; bash\n</code></pre>     or     <pre><code>srun --pty --jobid &lt;jobid&gt; --w &lt;nodename&gt; bash\n</code></pre></p> </li> <li> <p>Testing on LUMI-D:     <pre><code>srun -plumid -t30:00 --gres=gpu:1 --pty bash\n</code></pre></p> </li> </ul>"},{"location":"4_AI_packages/","title":"AI software on LUMI","text":""}]}