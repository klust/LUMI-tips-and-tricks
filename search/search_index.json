{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Information that is missing or hard to find in the Cray PE manual or LUMI docs","text":"<p>This unofficial site contains things I discovered in the HPE-Cray PE or other software on LUMI but have trouble finding in the documentation or is too specialised or anecdotal to have a place in the documentation.</p> <ol> <li>Compilers<ol> <li>Documentation for the compilers</li> <li>Miscellaneous topics</li> </ol> </li> <li>LMOD<ol> <li>LMOD documentation</li> <li>Lmod setup</li> <li>PE module structure</li> </ol> </li> <li>Slurm<ol> <li>Doc links</li> <li>Miscellaneous tips&amp;tricks</li> </ol> </li> <li>AI software</li> <li>MPI<ol> <li>Cray MPICH tips&amp;tricks</li> <li>Open MPI</li> <li>OSU micro-benchmarks</li> </ol> </li> <li>Linux<ol> <li>File management</li> </ol> </li> </ol>"},{"location":"00_00_introduction/","title":"Introduction","text":"<p>This unofficial site contains things I discovered in the HPE-Cray PE but have trouble finding in the documentation (probably also because the documentation is fairly hard to find on the web).</p>"},{"location":"01_Compilers/","title":"Compilers","text":"<ul> <li>Documentation for the compilers</li> <li>Miscellaneous topics</li> </ul>"},{"location":"01_Compilers/01_01_Doclinks/","title":"Documentation for the compilers","text":""},{"location":"01_Compilers/01_99_misc/","title":"Miscellaneous topics","text":""},{"location":"01_Compilers/01_99_misc/#using-the-compilers-without-the-wrappers","title":"Using the compilers without the wrappers","text":"Compiler module C compiler C++ compiler Fortran fixed format Fortran free format cce <code>craycc</code>, <code>clang</code> <code>crayCC</code>, <code>craycxx</code>, <code>clang++</code> <code>crayftn</code> <code>crayftn</code> gcc <code>gcc</code> <code>g++</code> <code>gfortran</code> <code>gfortran</code> aocc <code>clang</code> <code>clang++</code> <code>flang</code> <code>flang</code> rocm <code>amdclang</code> <code>amdclang++</code> <code>amdflang</code> <code>amdflang</code> <p>Remarks</p> <ul> <li> <p>The Cray clang compilers need to be pointed to the version of gcc to use using the     <code>--gcc-toolchain=&lt;value&gt;</code> option. The wrapper adds something like     <code>--gcc-toolchain=/opt/cray/pe/gcc/8.1.0/snos</code>.     Otherwise the compile will fail to find a suitable set of include files.</p> <p>TODO: Test this.</p> </li> <li> <p>A good trick to figure out which options the PE wrappers add is to use the     <code>-craype-verbose</code> flag on the command line of the wrappers.</p> </li> </ul>"},{"location":"02_LMOD/","title":"LMOD","text":"<ul> <li>Documentation links</li> <li>Lmod setup</li> <li>PE module structure</li> </ul>"},{"location":"02_LMOD/02_01_Doclinks/","title":"Documentation for LMOD","text":""},{"location":"02_LMOD/02_02_LMOD_setup/","title":"Cray Lmod setup","text":""},{"location":"02_LMOD/02_02_LMOD_setup/#2-versions-of-lmod","title":"2 versions of LMOD","text":"<p>Since the October-November 2023 update there are two versions of LMOD on the system</p> <ol> <li> <p>A version installed in the system directories, likely one that comes with SUSE linux.</p> <ul> <li>In November 2023 this is LMOD 8.3.1.</li> <li>Installation directory: <code>/usr/share/lmod/</code></li> </ul> </li> <li> <p>A version installed with the PE. </p> <ul> <li>In November 2023 this is LMOD 8.7.19 which comes with the 23.09 PE.</li> <li>Installation directory: <code>/opt/cray/pe/lmod</code></li> </ul> </li> </ol> <p>The active one after login is the one the comes with the HPE Cray PE, so when re-initialising be sure to call the initialisation functions of that version of LMOD.</p>"},{"location":"02_LMOD/02_02_LMOD_setup/#problems","title":"Problems","text":"<ul> <li> <p><code>module spider</code> does not work correctly. It does not always show all possible      combinations to access a module. This is because the HPE Cray PE doesn't use     hierarchies as intended by the LMOD developer. Sometimes a combination of two other     modules loaded in any order triggers a change to MODULEPATH.</p> </li> <li> <p>Noise about problems with the cache. We do note occasional cache problems on LUMI     but those don't seem to be associated with specific properties of the HPE Cray PE     Lmod setup but rather with the system not detecting that modules have been added     to the tree. I guess Linux with Lustre may have no notification system to detect     the moment of change in a subdirectory to see if a cache file is still valid and     to ignore (system cache) or regenerate (user cache) it otherwise.</p> </li> </ul>"},{"location":"02_LMOD/02_03_PE_module_structure/","title":"Custom paths","text":"<p>The information on this page is partly the result of studying <code>/opt/cray/pe/lmod_scripts/default/scripts/lmodHierarchy.lua</code> and other  module code.</p> <p>The HPE Cray PE can load custom paths with system-installed or user-installed  modules in a way that fits in the hierarchy defined by the PE. The following options are available:</p> <ul> <li> <p>Modules that depend on the CPU target module only.</p> <p>In the PE, these are in the <code>cpu</code> subdirectory and used for the <code>cray-fftw</code> modules.</p> <p>As these modules require loading only one module to be made available, there is no <code>handshake_</code> routine involved in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the network target module only, so not on the compiler or the     CPU architecture.</p> <p>In the PE, these are in the <code>net</code> subdirectory and used for the <code>cray-openshmemx</code> modules.</p> <p>As these modules require loading only one module to be made available, there is no <code>handshake_</code> routine involved in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the compiler only (so for a generic CPU)</p> <p>In the PE, these are in the <code>compiler</code> subdirectory and used for the <code>cray-hdf5</code>  modules.</p> <p>As these modules require loading only one module to be made available, there is no <code>handshake_</code> routine involved in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the CPU target module and the compiler.</p> <p>There are currently no such examples in the PE. however, the code is prepared for  future addition of such directories in the PE itself also and the reserved directory name is <code>comcpu</code>. </p> <p>As these modules require loading of two modules to be made available which in the PE can happen in any order as the hierarchy is not build in the typical Lmod way,  adding of both the PE and custom paths to <code>MODULEPATH</code> is managed by the <code>lmodHierarchy.handshake_comcpu</code> routine in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the network target module and the compiler.</p> <p>In the PE, these are in the <code>comnet</code> subdirectory and used for the <code>cray-mpich</code>, <code>cray-mpich-abi</code>, 'cray-mpich-ucx<code>and</code>cray-mpich-ucx-abi` modules.</p> <p>As these modules require loading of two modules to be made available which in the PE can happen in any order as the hierarchy is not build in the typical Lmod way,  adding of both the PE and custom paths to <code>MODULEPATH</code> is managed by the <code>lmodHierarchy.handshake_comnet</code> routine in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the network target module, the compiler and the MPI library.</p> <p>In the Cray PE, these are in the <code>mpi</code> subdirectory and used for the <code>cray-hdf5-parallel</code> and <code>cray-parallel-netcdf</code> modules.</p> <p>The addicion of the <code>mpi</code> subdiredctories is done in a way that is different from the other ones that depend on multiple modules, likely because the <code>cray-mpich-*</code> modules that add this level to the hierarchy themselves are at the <code>comnet</code> level and changed as soon as the network target or compiler module is changed, so that the regular Lmod approach of building a hierarchy can be used. The code that adds both the PE <code>mpi</code> directory and the corresponding custom path to the <code>MODULEPATH</code> is largely contained in  the <code>cray-mpich-*</code> modules but does use the <code>lmodHierarchy.get_user_custom_path</code> routine from <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on both the network and CPU target modules, on the compiler and      on the MPI library.</p> <p>There are currently no such examples in the PE. however, the code is prepared for  future addition of such directories in the PE itself also and the reserved directory name is <code>cncm</code>. </p> <p>As these modules require loading of two modules to be made available which in the PE can happen in any order as the hierarchy is not build in the typical Lmod way,  adding of both the PE and custom paths to <code>MODULEPATH</code> is managed by the <code>lmodHierarchy.handshake_cncm</code> routine in <code>lmodHierarchy.lua</code>.</p> </li> </ul> <pre><code>module_root\n\u251c\u2500 cpu\n\u2502  \u251c\u2500 x86-64\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u251c\u2500 x86-milan\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u251c\u2500 x86-rome\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u2514\u2500 x86-trento\n\u2502     \u2514\u2500 1.0\n\u251c\u2500 net\n\u2502  \u251c\u2500 ofi\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u2514\u2500 ucx\n\u2502     \u2514\u2500 1.0\n\u251c\u2500 compiler\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u251c\u2500 comcpu\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2502     \u251c\u2500 x86-64\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u251c\u2500 x86-milan\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u251c\u2500 x86-rome\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u2514\u2500 x86-trento\n\u2502  \u2502        \u2514\u2500 1.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u251c\u2500 comnet\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2502     \u251c\u2500 ofi\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u2514\u2500 ucx\n\u2502  \u2502        \u2514\u2500 1.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u251c\u2500 mpi\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2502     \u251c\u2500 ofi\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u2502     \u2514\u2500 cray-mpich\n\u2502  \u2502     \u2502        \u2514\u2500 8.0\n\u2502  \u2502     \u2514\u2500 ucx\n\u2502  \u2502        \u2514\u2500 1.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u2514\u2500 cncm\n   \u251c\u2500 aocc\n   \u2502  \u251c\u2500 2.2\n   \u2502  \u2514\u2500 3.0\n   \u251c\u2500 crayclang\n   \u2502  \u2514\u2500 10.0\n   \u2502     \u251c\u2500 ofi\n   \u2502     \u2502  \u2514\u2500 1.0\n   \u2502     \u2502     \u251c\u2500 x86-64\n   \u2502     \u2502     \u2502  \u2514\u2500 1.0\n   \u2502     \u2502     \u2502     \u2514\u2500 cray-mpich\n   \u2502     \u2502     \u2502        \u2514\u2500 8.0\n   \u22ee     \u22ee      \u22ee   \n   \u2502     \u2514\u2500 ucx\n   \u2502        \u2514\u2500 1.0\n   \u2514\u2500 gnu\n      \u2514\u2500 8.0\n</code></pre> <p>Given that the custom paths are not set by a single environment variable pointing to the roots of the custom installation directories and imposing the same directory structure as used by the PE, but instead by separate environment variables for every leaf in the picture below, the number of environment variables can explode.</p> <p>On LUMI, as of July 2022, we have:</p> <ul> <li>3 CPU target modules and the generic one if we want to support that also,     so 4 CPU target modules.</li> <li>1 network library (though one could argue that UCX can still be used on the      login, large memory and visualisation GPU nodes but these are not really meant     for heavy MPI work unless they are integrating with jobs on the regular compute      nodes)</li> <li>4 compiler ABIs, with one to be dropped soon (aocc/2.2, aocc/3.0, crayclang/10.0 and gnu/8.0)</li> <li>1 MPI ABI version (cray-mpich/8.0)</li> </ul> <p>This may result in:</p> <ul> <li>4 <code>LMOD_CUSTOM_CPU_*</code> environment variables</li> <li>1 <code>LMOD_CUSTOM_NET_*</code> environment variable (or 2 with UCX)</li> <li>4 <code>LMOD_CUSTOM_COMPILER_*</code> environment variables</li> <li>16 <code>LMOD_CUSTOM_COMCPU_*</code> environment variables</li> <li>4 <code>LMOD_CUSTOM_COMNET_*</code> environment variables (8 with UCX)</li> <li>4 <code>LMOD_CUSTOM_MPI_*</code> environment variables (8 with UCX)</li> <li>16 <code>LMOD_CUSTOM_CNCM_*</code> environment variables (24 with UCX as two CPU architectures don't make sense with UCX     on LUMI)</li> </ul> <p>The names of the environment variables are derived from the above directory structure, but</p> <ul> <li>All characters uppercase</li> <li><code>/</code>, <code>-</code>and <code>.</code> get substituted by an underscore character</li> <li>The resulting name is prefixed with <code>LMOD_CUSTOM_</code> and postfixed with <code>_PREFIX</code>.</li> </ul>"},{"location":"03_Slurm/","title":"Slurm on LUMI","text":"<ul> <li>Doc links</li> <li>Miscellaneous tips&amp;tricks</li> </ul>"},{"location":"03_Slurm/03_01_Doclinks/","title":"Slurm documentation links","text":"<p>Note: Check the version of Slurm after each LUMI update.</p> <p>The version after the October-November 2023 update is 22.05.10.</p>"},{"location":"03_Slurm/03_01_Doclinks/#web-documentation","title":"Web documentation","text":"<ul> <li> <p>Slurm version 22.05.10, on the system at the time of the course</p> </li> <li> <p>Slurm manual pages are also all on the web      and are easily found by Google, but are usually those for the latest version.</p> <ul> <li> <p><code>man sbatch</code></p> </li> <li> <p><code>man srun</code></p> </li> <li> <p><code>man salloc</code></p> </li> <li> <p><code>man squeue</code></p> </li> <li> <p><code>man scancel</code></p> </li> <li> <p><code>man sinfo</code></p> </li> <li> <p><code>man sstat</code></p> </li> <li> <p><code>man sacct</code></p> </li> <li> <p><code>man scontrol</code></p> </li> </ul> </li> </ul>"},{"location":"03_Slurm/03_99_misc/","title":"Miscellaneous Slurm tips&amp;trics","text":""},{"location":"03_Slurm/03_99_misc/#system-configuration","title":"System configuration","text":"<ul> <li> <p>Extensive information about the partitions:     <pre><code>scontrol show partition --all\n</code></pre>     With `--all`` to also see hidden partitions.</p> </li> <li> <p>Show topology:     <pre><code>scontrol show topology\n</code></pre></p> </li> <li> <p>Show extensive configuration info:     <pre><code>scontrol show config\n</code></pre></p> </li> <li> <p>Also interesting:     <pre><code>scontrol show assoc_mgr\n</code></pre></p> </li> </ul>"},{"location":"03_Slurm/03_99_misc/#user-or-project-related","title":"User- or project-related","text":"<ul> <li> <p>Show which partitions a project has access to:     <pre><code>sacctmgr show assoc where account=project_462000087\n</code></pre></p> </li> <li> <p>Show which partitions a user has access to:     <pre><code>sacctmgr show assoc format=account,Partition  -p Users=kurtlust\n</code></pre></p> </li> </ul>"},{"location":"03_Slurm/03_99_misc/#job-management","title":"Job management","text":"<ul> <li>History of slurm jobs:<ul> <li>Through <code>sacct</code> <pre><code>sacct -a\n</code></pre></li> <li>Through <code>slurm</code> <pre><code>slurm history 2hours -a \n</code></pre>     or from a particular user:     <pre><code>slurm history 1day -u &lt;username&gt;\n</code></pre></li> </ul> </li> </ul>"},{"location":"03_Slurm/03_99_misc/#misc","title":"Misc","text":"<ul> <li> <p>Attach to a running job rather than ssh into a node:     <pre><code>srun --pty --jobid &lt;jobid&gt; bash\n</code></pre>     or     <pre><code>srun --pty --jobid &lt;jobid&gt; --w &lt;nodename&gt; bash\n</code></pre></p> </li> <li> <p>Testing on LUMI-D:     <pre><code>srun -plumid -t30:00 --gres=gpu:1 --pty bash\n</code></pre></p> </li> </ul>"},{"location":"04_AI_packages/","title":"AI software on LUMI","text":""},{"location":"05_MPI/","title":"MPI","text":"<ul> <li> <p>Cray MPICH tips&amp;tricks</p> </li> <li> <p>Open MPI</p> </li> <li> <p>OSU micro-benchmarks</p> </li> </ul>"},{"location":"05_MPI/05_01_Cray_MPICH_tips/","title":"Cray MPICH tips","text":""},{"location":"05_MPI/05_01_Cray_MPICH_tips/#remarks-from-the-november-2023-hackathon","title":"Remarks from the November 2023 hackathon","text":"<ul> <li> <p>Using hugepages can improve intra-node performance (or performance overall?)</p> </li> <li> <p>Intra-node performance can improve by setting</p> <pre><code>export MPICH_SMP_SINGLE_COPY_MODE=NONE\n</code></pre> <p>at least for large message sizes.</p> </li> </ul>"},{"location":"05_MPI/05_02_OpenMPI/","title":"Open MPI on LUMI","text":""},{"location":"05_MPI/05_02_OpenMPI/#build-instructions-alfio-from-the-hackathon","title":"Build instructions Alfio from the hackathon","text":"<pre><code>module load PrgEnv-gnu\nmodule unload cray-mpich\nOMPI_VERSION=4.1.6\necho \"Install OpenMPI v\"$OMPI_VERSION\nwget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-$OMPI_VERSION.tar.bz2\ntar xf openmpi-$OMPI_VERSION.tar.bz2 &amp;&amp; rm openmpi-$OMPI_VERSION.tar.bz2\ncd openmpi-$OMPI_VERSION\n\n# SLURM (default yes), OFI, PMI\nmake clean\n./configure --prefix=$PWD/install_ofi_slurm_pmi/ \\\n--with-ofi=/opt/cray/libfabric/1.15.2.0 --with-slurm --with-pmi=/usr CC=gcc CXX=g++ FTN=gfortran CFLAGS=\"-march=znver2\" \\\n--disable-shared --enable-static\nmake -j 10 install\n\nrm -f install\nln -s $PWD/install_ofi_slurm_pmi install\n\nmodule unload cray-mpich\nexport PATH=$PWD/install/bin/:$PATH\nexport LD_LIBRARY_PATH=$PWD/install/lib:$LD_LIBRARY_PATH\n\n# Test with OSU\nwget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-5.9.tar.gz\ntar xf osu-micro-benchmarks-5.9.tar.gz\nrm osu-micro-benchmarks-5.9.tar.gz\ncd osu-micro-benchmarks-5.9\n\n./configure --prefix=$PWD/install CC=$(which mpicc) CXX=$(which mpicxx) CFLAGS=-O3 CXXFLAGS=-O3\nmake -j 10 install\n\ncd install/libexec/osu-micro-benchmarks/mpi/p2p \nsrun -n 2 --ntasks-per-node=1 ./osu_bw\n\n# Cannot remember the performance, try to compare to cray-mpich\n</code></pre> <p>It looks like you can now safely build shared libraries though.</p> <p>Useful links:</p> <ul> <li>Use at NERSC</li> </ul>"},{"location":"05_MPI/05_02_OpenMPI/#open-mpi-5-for-slingshot-11","title":"Open MPI 5 for Slingshot 11","text":"<p>Useful links:</p> <ul> <li> <p>CUG paper ORNL</p> <p>The paper says that changes needed in Open MPI itself are present in the Open MPI main and v5.0.x branches, but not in branches for older versions.</p> <p>One problem with Open MPI on OFI+CXI was that the CXI provider has no special pathway for intra-node messaging. Rather than redeveloping the MTL to deal with multiple components concurrently, a new provider was made, LINKx, encapsulating CXI but adding the missing features.</p> </li> <li> <p>Use at NERSC</p> </li> </ul>"},{"location":"05_MPI/05_03_OSU_benchmarks/","title":"OSU micro-benchmarks","text":"<ul> <li> <p>Home page of the benchmarks</p> </li> <li> <p>Examples of running the benchmarks:</p> <ul> <li> <p>University of Luxemburg HPC tutorial page</p> </li> <li> <p>AWS HPC workshop</p> </li> <li> <p>Sigma2 page</p> </li> </ul> </li> </ul>"},{"location":"99_Linux/","title":"Rather more general Linux tips","text":"<ul> <li>File management</li> </ul>"},{"location":"99_Linux/99_01_File_management/","title":"File management tips and tricks","text":"<ul> <li> <p>How to quickly check the number of files?</p> <ul> <li>Solution with <code>rsync</code>:     <pre><code>rsync --stats --dry-run\n</code></pre></li> </ul> </li> </ul>"}]}