{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Miscellaneous LUMI information, tips and tricks","text":"<p>This unofficial site contains things I discovered in the HPE-Cray PE or other software on LUMI but have trouble finding in the documentation or is too specialised or anecdotal to have a place in the documentation.</p> <p>This is by no means an official LUMI documentation site. It is strictly the personal project of Kurt Lust and his memory of certain things. In no means does it express any opinion of my employer, the University of Antwerp, nor from any other entity involved in the LUMI project.</p> <p>Documentation links page maintained for the course pages</p> <ol> <li>Compilers<ol> <li>Documentation for the compilers</li> <li>LibSci</li> <li>Wrapper-induced problems</li> <li>Performance tools and metrics</li> <li>Intel compilers and libraries</li> <li>Miscellaneous topics</li> </ol> </li> <li>LMOD<ol> <li>LMOD documentation</li> <li>Lmod setup</li> <li>PE module structure</li> </ol> </li> <li>Slurm<ol> <li>Doc links</li> <li>GPU binding</li> <li>Slurm in a container</li> <li>Miscellaneous tips&amp;tricks</li> </ol> </li> <li>AI software<ol> <li>TensorFlow</li> </ol> </li> <li>MPI<ol> <li>Cray MPICH tips&amp;tricks</li> <li>Open MPI</li> <li>OSU micro-benchmarks</li> <li>Open MPI tips&amp;tricks</li> <li>Misc</li> </ol> </li> <li>ROCm<ol> <li>Miscellaneous</li> </ol> </li> <li>Lustre<ol> <li>Miscellaneous topics: Lustre</li> </ol> </li> <li>LUMI-O<ol> <li>Miscellaneous topics: LUMI-O</li> </ol> </li> <li>Software<ol> <li>GROMACS</li> <li>ParaView</li> <li>Python</li> <li>R</li> </ol> </li> <li>Linux<ol> <li>File management</li> <li>Bash tips</li> </ol> </li> <li>Misc<ol> <li>LUMI flop computation</li> <li>Login node organisation</li> <li>HPE software releases</li> <li>EESSI</li> <li>Energy consumption</li> <li>Answers to frequent user complaints</li> </ol> </li> </ol>"},{"location":"00_00_introduction/","title":"Introduction","text":"<p>This unofficial site contains things I discovered in the HPE-Cray PE but have trouble finding in the documentation (probably also because the documentation is fairly hard to find on the web).</p> <p>It has later been extended with many other tips and tricks about LUMI.</p>"},{"location":"Doclinks/","title":"Documentation links","text":"<p>Note that documentation, and especially web based documentation, is very fluid. Links change rapidly and were correct when this page was developed right after the course. However, there is no guarantee that they are still correct when you read this and will only be updated at the next course on the pages of that course.</p> <p>This documentation page is far from complete but bundles a lot of links mentioned during the presentations, and some more.</p>"},{"location":"Doclinks/#web-documentation","title":"Web documentation","text":"<ul> <li> <p>HPE Cray Programming Environment web documentation has only become available in      May 2023 and is a work-in-progress. It does contain a lot of HTML-processed man pages in an easier-to-browse      format than the man pages on the system.</p> <p>The presentations on debugging and profiling tools referred a lot to pages that can be found on this web site.  The manual pages mentioned in those presentations are also in the web documentation and are the easiest way  to access that documentation.</p> </li> <li> <p>Cray PE Github account with whitepapers and some documentation.</p> </li> <li> <p>Cray DSMML - Distributed Symmetric Memory Management Library</p> </li> <li> <p>Cray Library previously provides as TPSL build instructions</p> </li> <li> <p>Clang latest version documentation (Usually for the latest version)</p> <ul> <li> <p>Clang 13.0.0 version (basis for aocc/3.2.0)</p> </li> <li> <p>Clang 14.0.0 version (basis for rocm/5.2.3 and amd/5.2.3)</p> </li> <li> <p>Clang 15.0.0 version (cce/15.0.0 and cce/15.0.1 in 22.12/23.03)</p> </li> <li> <p>Clang 16.0.0 version (cce/16.0.0 in 23.09)</p> </li> </ul> </li> <li> <p>AMD Developer Information. Note that AMD doesn't archive     manuals of older versions which can be a problem. You have to reprocess them from GitHub repositories.</p> <ul> <li> <p>AOCC 4.0 CompilerOptions Quick Reference Guide      (Version 4.0 compilers will come when the 23.05 or later CPE release gets installed on LUMI and the system      is updated to COS 2.5 as some libraries are missing in COS 2.4)</p> </li> <li> <p>AOCC 4.0 User Guide</p> </li> </ul> </li> <li> <p>ROCm<sup>TM</sup> documentation overview</p> <ul> <li> <p>rocminfo application for reporting system info.</p> </li> <li> <p>rocm-smi</p> </li> <li> <p>HIP porting guide</p> </li> <li> <p>ROCm Software Platform GitHub repository</p> </li> <li> <p>Libraries:</p> <ul> <li> <p>BLAS: rocBLAS and hipBLAS</p> </li> <li> <p>FFTs: rocFFT and hipFFT</p> </li> <li> <p>Random number generation: rocRAND</p> </li> <li> <p>Sparse linear algebra: rocSPARSE and hipSPARSE</p> </li> <li> <p>Iterative solvers: rocALUTION</p> </li> <li> <p>Parallel primitives: rocPRIM and hipCUB</p> </li> <li> <p>Machine Learning Libraries: MIOpen (similar to cuDNN),      Tensile (GEMM Autotuner),     RCCL (ROCm analogue of NCCL) and      Horovod (Distributed ML)</p> </li> <li> <p>Machine Learning Frameworks: Tensorflow,     Pytorch and     Caffe</p> </li> <li> <p>Machine Learning Benchmarks:     DeepBench and      MLPerf</p> </li> </ul> </li> <li> <p>Development tools:</p> <ul> <li> <p>rocgdb resources:</p> <ul> <li> <p>AMD documentation</p> </li> <li> <p>2021 presentation by Justin Chang</p> </li> <li> <p>2021 Linux Plumbers Conference presentation     with youTube video with a part of the presentation</p> </li> </ul> </li> <li> <p>rocprof profiler</p> </li> <li> <p>OmniTrace</p> </li> <li> <p>Omniperf</p> </li> </ul> </li> </ul> </li> <li> <p>HDF5 generic documentation</p> </li> <li> <p>Mentioned in the Lustre presentation: The      ExaIO project paper     \"Transparent Asynchronous Parallel I/O Using Background Threads\".</p> </li> </ul> AMD documentation <p>AMD doesn't archive documentation for past versions of ROCM and their CPU compilers in a way that is ready-to-read. Instead </p>"},{"location":"Doclinks/#man-pages","title":"Man pages","text":"<p>A selection of man pages explicitly mentioned during the course:</p> <ul> <li> <p>Compilers</p> PrgEnv C C++ Fortran PrgEnv-cray <code>man craycc</code> <code>man crayCC</code> <code>man crayftn</code> PrgEnv-gnu <code>man gcc</code> <code>man g++</code> <code>man gfortran</code> PrgEnv-aocc/PrgEnv-amd - - - Compiler wrappers <code>man cc</code> <code>man CC</code> <code>man ftn</code> </li> <li> <p>OpenMP in CCE</p> <ul> <li><code>man intro_openmp</code></li> </ul> </li> <li> <p>OpenACC in CCE</p> <ul> <li><code>man intro_openacc</code></li> </ul> </li> <li> <p>MPI:</p> <ul> <li> <p>MPI itself: <code>man intro_mpi</code> or <code>man mpi</code></p> </li> <li> <p>libfabric: <code>man fabric</code></p> </li> <li> <p>CXI: `man fi_cxi'</p> </li> </ul> </li> <li> <p>LibSci</p> <ul> <li> <p><code>man intro_libsci</code> and <code>man intro_libsci_acc</code></p> </li> <li> <p><code>man intro_blas1</code>,     <code>man intro_blas2</code>,     <code>man intro_blas3</code>,     <code>man intro_cblas</code></p> </li> <li> <p><code>man intro_lapack</code></p> </li> <li> <p><code>man intro_scalapack</code> and <code>man intro_blacs</code></p> </li> <li> <p><code>man intro_irt</code></p> </li> <li> <p><code>man intro_fftw3</code></p> </li> </ul> </li> <li> <p>DSMML - Distributed Symmetric Memory Management Library </p> <ul> <li><code>man intro_dsmml</code></li> </ul> </li> <li> <p>Slurm manual pages are also all on the web      and are easily found by Google, but are usually those for the latest version.</p> <ul> <li> <p><code>man sbatch</code></p> </li> <li> <p><code>man srun</code></p> </li> <li> <p><code>man salloc</code></p> </li> <li> <p><code>man squeue</code></p> </li> <li> <p><code>man scancel</code></p> </li> <li> <p><code>man sinfo</code></p> </li> <li> <p><code>man sstat</code></p> </li> <li> <p><code>man sacct</code></p> </li> <li> <p><code>man scontrol</code></p> </li> </ul> </li> </ul>"},{"location":"Doclinks/#via-the-module-system","title":"Via the module system","text":"<p>Most HPE Cray PE modules contain links to further documentation. Try <code>module help cce</code> etc.</p>"},{"location":"Doclinks/#from-the-commands-themselves","title":"From the commands themselves","text":"PrgEnv C C++ Fortran PrgEnv-cray <code>craycc --help</code> <code>crayCC --help</code> <code>crayftn --help</code> <code>craycc --craype-help</code> <code>crayCC --craype-help</code> <code>crayftn --craype-help</code> PrgEnv-gnu <code>gcc --help</code> <code>g++ --help</code> <code>gfortran --help</code> PrgEnv-aocc <code>clang --help</code> <code>clang++ --help</code> <code>flang --help</code> PrgEnv-amd <code>amdclang --help</code> <code>amdclang++ --help</code> <code>amdflang --help</code> Compiler wrappers <code>cc --help</code> <code>CC --help</code> <code>ftn --help</code> <p>For the PrgEnv-gnu compiler, the <code>--help</code> option only shows a little bit of help information, but mentions further options to get help about specific topics.</p> <p>Further commands that provide extensive help on the command line:</p> <ul> <li><code>rocm-smi --help</code>, even on the login nodes.</li> </ul>"},{"location":"Doclinks/#documentation-of-other-cray-ex-systems","title":"Documentation of other Cray EX systems","text":"<p>Note that these systems may be configured differently, and this especially applies to the scheduler. So not all documentations of those systems applies to LUMI. Yet these web sites do contain a lot of useful information.</p> <ul> <li> <p>Archer2 documentation.      Archer2 is the national supercomputer of the UK, operated by EPCC. It is an AMD CPU-only cluster.     Two important differences with LUMI are that (a) the cluster uses AMD Rome CPUs with groups of 4 instead     of 8 cores sharing L3 cache and (b) the cluster uses Slingshot 10 instead of Slinshot 11 which has its     own bugs and workarounds.</p> <p>It includes a page on cray-python referred to during the course.</p> </li> <li> <p>ORNL Frontier User Guide and      ORNL Crusher Qucik-Start Guide.     Frontier is the first USA exascale cluster and is built up of nodes that are very similar to the     LUMI-G nodes (same CPA and GPUs but a different storage configuration) while Crusher is the     192-node early access system for Frontier. One important difference is the configuration of     the scheduler which has 1 core reserved in each CCD to have a more regular structure than LUMI.</p> </li> <li> <p>KTH Dardel documentation. Dardel is the Swedish \"baby-LUMI\" system.     Its CPU nodes use the AMD Rome CPU instead of AMD Milan, but its GPU nodes are the same as in LUMI.</p> </li> <li> <p>Setonix User Guide.     Setonix is a Cray EX system at Pawsey Supercomputing Centre in Australia. The CPU and GPU compute     nodes are the same as on LUMI.</p> </li> </ul>"},{"location":"01_Compilers/","title":"Compilers","text":"<ul> <li>Documentation for the compilers</li> <li>LibSci</li> <li>Wrapper-induced problems</li> <li>Performance tools and metrics</li> <li>Intel compilers and libraries</li> <li>Miscellaneous topics</li> </ul>"},{"location":"01_Compilers/01_01_Doclinks/","title":"Documentation for the compilers","text":""},{"location":"01_Compilers/01_02_LibSci/","title":"LibSci","text":"<ul> <li> <p>Cray LibSci only uses 32-bit integers in BLAS, LAPACK and ScaLAPACK. There is     no version with 64-bit integers at the moment.</p> <p>(ticket #3674)</p> </li> </ul>"},{"location":"01_Compilers/01_03_WrapperIssues/","title":"Wrapper-induced issues","text":""},{"location":"01_Compilers/01_03_WrapperIssues/#tests-for-score-p-failing-on-the-gpu-nodes","title":"Tests for Score-P failing on the GPU nodes","text":"<p>Reproducing is trivial:</p> <pre><code>#include &lt;stdlib.h&gt;\n#include &lt;omp.h&gt;\n#include &lt;inttypes.h&gt;\n\nint main( void )\n{\n    return 0;\n}\n</code></pre> <p>and compile with <code>LUMI/23.09 partition/G cpeCray</code> or <code>cpeAMD</code>.</p> <p>Answer from Alfio:</p> <ul> <li> <p>The problem when using the wrapper <code>cc</code> is that the module craype-accel-amd-gfx90a promotes OpenMP to be offloaded to the target GPU.      Then, we have to mix with the ROCm files and this can be done after the preprocessing of the OpenMP pragmas.</p> <p>You can use the flag <code>-craype-verbose</code> to see what command is executed, e.g.</p> <pre><code>$  cc -fopenmp -craype-verbose --preprocess test_with_includes.c\n\nclang -march=znver2 -fopenmp-targets=amdgcn-amd-amdhsa -Xopenmp-target=amdgcn-amd-amdhsa -march=gfx90a -dynamic -D__CRAY_X86_ROME -D__CRAY_AMD_GFX90A -D__CRAYXT_COMPUTE_LINUX_TARGET --gcc-toolchain=/opt/cray/pe/gcc/10.3.0/snos -isystem /opt/cray/pe/cce/16.0.1/cce-clang/x86_64/lib/clang/16/include -isystem /opt/cray/pe/cce/16.0.1/cce/x86_64/include/craylibs -Wl,-rpath=/opt/cray/pe/cce/16.0.1/cce/x86_64/lib -fopenmp --preprocess test_with_includes.c -I/opt/cray/pe/libsci/23.09.1.1/CRAY/12.0/x86_64/include -I/opt/cray/pe/mpich/8.1.27/ofi/cray/14.0/include -I/opt/cray/pe/dsmml/0.2.2/dsmml//include -I/opt/cray/xpmem/2.5.2-2.4_3.50__gd0f7936.shasta/include -L/opt/cray/pe/libsci/23.09.1.1/CRAY/12.0/x86_64/lib -L/opt/cray/pe/mpich/8.1.27/ofi/cray/14.0/lib -L/opt/cray/pe/mpich/8.1.27/gtl/lib -L/opt/cray/pe/dsmml/0.2.2/dsmml//lib -L/opt/cray/pe/cce/16.0.1/cce/x86_64/lib/pkgconfig/../ -L/opt/cray/xpmem/2.5.2-2.4_3.50__gd0f7936.shasta/lib64 -Wl,--as-needed,-lsci_cray_mpi_mp,--no-as-needed -Wl,--as-needed,-lsci_cray_mp,--no-as-needed -ldl -Wl,--as-needed,-lmpi_cray,--no-as-needed -lmpi_gtl_hsa -Wl,--as-needed,-ldsmml,--no-as-needed -lxpmem -Wl,--as-needed,-lstdc++,--no-as-needed -Wl,--as-needed,-lpgas-shmem,--no-as-needed -lquadmath -lmodules -lfi -lcraymath -lf -lu -lcsup -Wl,--as-needed,-lpthread,-latomic,--no-as-needed -Wl,--as-needed,-lm,--no-as-needed -Wl,--disable-new-dtags \n</code></pre> <p>As you said, it works if you don't directly use the wrappers (or you remove OpenMP, or if you don't load the GPU target module). </p> <p>My suspicious is that the problem is due to LLVM (&lt;=16) compatibility.</p> </li> <li> <p>For the CCE,  we can apply the following trick:</p> <pre><code>cc -fno-cray -fopenmp --preprocess test_with_includes.c &gt; test_with_includes.prep.offload.c\n</code></pre> <p>Here, the <code>-fno-cray</code> gives you the vanilla LLVM compiler without specific Cray optimizations. Note that we need this trick only for the CCE 16.0.1 installed on LUMI, I tried with CCE 17+ and we don't need this flag anymore.</p> <p>The next step is to compile the preprocessor output:</p> <pre><code>$ cc -fno-cray -fopenmp --preprocess test_with_includes.c &gt; test_with_includes.prep.offload.c &amp;&amp; cc -nostdinc -fopenmp test_with_includes.prep.offload.c\n</code></pre> <p>In this case I'm using the flag <code>-nostdinc</code> to avoid the compiler to reintroduce implicit headers that will conflict with the existing headers of the preprocessor output.</p> </li> <li> <p>Concerning the AMD compiler, the version installed on LUMI relies on LLVM 14 (Rocm 5.2) and it gives you the same problem      (even if you specify <code>-nostdinc</code>). Also in this case, I've tested it with a new ROCM version (6.1) and it works.      I don't have a workaround for the version installed on LUMI though, the suggestion can be to directly use amdclang as      you already did (you can use the flag <code>--cray-print-opts</code> for the wrappers to get a list of the options and then      exclude the OpenMP offload ones, see <code>man cc</code>).</p> </li> </ul>"},{"location":"01_Compilers/01_04_PerformanceTools/","title":"Performance tools and metrics","text":""},{"location":"01_Compilers/01_04_PerformanceTools/#performance-counters","title":"Performance counters","text":"<p>Partly based on messages from Orian in the chat:</p> <ul> <li> <p>Check which ones are available through PAPI: <code>papi_avail</code>.</p> </li> <li> <p>Any L3 events are disabled as they are \"uncore\" events so that there is a security     risk as they have to do with shared resources.      Uncore events require <code>/proc/sys/kernel/perf_event_paranoid</code> value to be set to 0 or -1     which is not the case on LUMI.</p> </li> <li> <p>There is a difference between counters on zen2 and zen3, so checking on the login nodes      does not show what you can use on the LUMI-C and LUMI-G compute nodes...</p> </li> </ul>"},{"location":"01_Compilers/01_05_Intel/","title":"Intel compilers and libraries","text":"<ul> <li>Interesting remark on using MKL on Zen</li> </ul>"},{"location":"01_Compilers/01_99_misc/","title":"Miscellaneous topics","text":""},{"location":"01_Compilers/01_99_misc/#using-the-compilers-without-the-wrappers","title":"Using the compilers without the wrappers","text":"Compiler module C compiler C++ compiler Fortran fixed format Fortran free format cce <code>craycc</code>, <code>clang</code> <code>crayCC</code>, <code>craycxx</code>, <code>clang++</code> <code>crayftn</code> <code>crayftn</code> gcc <code>gcc</code> <code>g++</code> <code>gfortran</code> <code>gfortran</code> gcc-native <code>gcc-12</code>, etc. <code>g++-12</code>, etc. <code>gfortran-12</code>, etc. <code>gfortran-12</code>, etc. aocc <code>clang</code> <code>clang++</code> <code>flang</code> <code>flang</code> rocm <code>amdclang</code> <code>amdclang++</code> <code>amdflang</code> <code>amdflang</code> <p>Remarks</p> <ul> <li> <p>The Cray clang compilers need to be pointed to the version of gcc to use using the     <code>--gcc-toolchain=&lt;value&gt;</code> option. The wrapper adds something like     <code>--gcc-toolchain=/opt/cray/pe/gcc/8.1.0/snos</code>.     Otherwise the compile will fail to find a suitable set of include files.</p> <p>TODO: Test this.</p> </li> <li> <p>A good trick to figure out which options the PE wrappers add is to use the     <code>-craype-verbose</code> flag on the command line of the wrappers.</p> </li> </ul>"},{"location":"01_Compilers/01_99_misc/#compiler-versions","title":"Compiler versions","text":"CPE cce gcc aocc amd 21.08 12.0.2 <code>gcc/10.3.0</code> 3.0.0 4.5.2 21.12 13.0.0 <code>gcc/11.2.0</code> 3.1.0 4.5.2 22.08 14.0.2 <code>gcc/11.2.0</code> 3.2.0 5.0.2 22.12 15.0.0 <code>gcc/11.2.0</code> 3.2.0 5.2.3 23.03 15.0.1 <code>gcc/11.2.0</code> 3.2.0 5.2.3 23.09 16.0.1 <code>gcc/12.2.0</code> 3.2.0 5.2.3 23.12 17.0.0 <code>gcc-native/12.3</code> 4.1.0 6.0.3 24.03 17.0.1 <code>gcc-native/13.2</code> 4.1.0 6.0.3"},{"location":"01_Compilers/01_99_misc/#finding-the-include-files-etc-for-hipcc-when-compiling-other-code-with-the-cray-compiler","title":"Finding the include files etc. for <code>hipcc</code> when compiling other code with the Cray compiler","text":"<pre><code>export HIPCC_COMPILE_FLAGS_APPEND=\"--offload-arch=gfx90a $(CC --cray-print-opts=cflags)\"\nexport HIPCC_LINK_FLAGS_APPEND=$(CC --cray-print-opts=libs)\n</code></pre>"},{"location":"01_Compilers/01_99_misc/#compilers-compared","title":"Compilers compared","text":"<ul> <li>GNU Fortran, old and new flang:<ul> <li>Page in the Linaro blog (August 24, 2023)     but the results of the benchmarks are on Arm.</li> </ul> </li> </ul>"},{"location":"02_LMOD/","title":"LMOD","text":"<ul> <li>Documentation links</li> <li>Lmod setup</li> <li>PE module structure</li> </ul>"},{"location":"02_LMOD/02_01_Doclinks/","title":"Documentation for LMOD","text":""},{"location":"02_LMOD/02_02_LMOD_setup/","title":"Cray Lmod setup","text":""},{"location":"02_LMOD/02_02_LMOD_setup/#2-versions-of-lmod","title":"2 versions of LMOD","text":"<p>Since the October-November 2023 update there are two versions of LMOD on the system</p> <ol> <li> <p>A version installed in the system directories, likely one that comes with SUSE linux.</p> <ul> <li>In November 2023 this is LMOD 8.3.1.</li> <li>Installation directory: <code>/usr/share/lmod/</code></li> </ul> </li> <li> <p>A version installed with the PE. </p> <ul> <li>In November 2023 this is LMOD 8.7.19 which comes with the 23.09 PE.</li> <li>Installation directory: <code>/opt/cray/pe/lmod</code></li> </ul> </li> </ol> <p>The active one after login is the one the comes with the HPE Cray PE, so when re-initialising be sure to call the initialisation functions of that version of LMOD.</p>"},{"location":"02_LMOD/02_02_LMOD_setup/#problems","title":"Problems","text":"<ul> <li> <p><code>module spider</code> does not work correctly. It does not always show all possible      combinations to access a module. This is because the HPE Cray PE doesn't use     hierarchies as intended by the LMOD developer. Sometimes a combination of two other     modules loaded in any order triggers a change to MODULEPATH.</p> </li> <li> <p>Noise about problems with the cache. We do note occasional cache problems on LUMI     but those don't seem to be associated with specific properties of the HPE Cray PE     Lmod setup but rather with the system not detecting that modules have been added     to the tree. I guess Linux with Lustre may have no notification system to detect     the moment of change in a subdirectory to see if a cache file is still valid and     to ignore (system cache) or regenerate (user cache) it otherwise.</p> </li> </ul>"},{"location":"02_LMOD/02_03_PE_module_structure/","title":"Custom paths","text":"<p>The information on this page is partly the result of studying <code>/opt/cray/pe/lmod_scripts/default/scripts/lmodHierarchy.lua</code> and other  module code.</p> <p>The HPE Cray PE can load custom paths with system-installed or user-installed  modules in a way that fits in the hierarchy defined by the PE. The following options are available:</p> <ul> <li> <p>Modules that depend on the CPU target module only.</p> <p>In the PE, these are in the <code>cpu</code> subdirectory and used for the <code>cray-fftw</code> modules.</p> <p>As these modules require loading only one module to be made available, there is no <code>handshake_</code> routine involved in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the network target module only, so not on the compiler or the     CPU architecture.</p> <p>In the PE, these are in the <code>net</code> subdirectory and used for the <code>cray-openshmemx</code> modules.</p> <p>As these modules require loading only one module to be made available, there is no <code>handshake_</code> routine involved in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the compiler only (so for a generic CPU)</p> <p>In the PE, these are in the <code>compiler</code> subdirectory and used for the <code>cray-hdf5</code>  modules.</p> <p>As these modules require loading only one module to be made available, there is no <code>handshake_</code> routine involved in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the CPU target module and the compiler.</p> <p>There are currently no such examples in the PE. however, the code is prepared for  future addition of such directories in the PE itself also and the reserved directory name is <code>comcpu</code>. </p> <p>As these modules require loading of two modules to be made available which in the PE can happen in any order as the hierarchy is not build in the typical Lmod way,  adding of both the PE and custom paths to <code>MODULEPATH</code> is managed by the <code>lmodHierarchy.handshake_comcpu</code> routine in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the network target module and the compiler.</p> <p>In the PE, these are in the <code>comnet</code> subdirectory and used for the <code>cray-mpich</code>, <code>cray-mpich-abi</code>, 'cray-mpich-ucx<code>and</code>cray-mpich-ucx-abi` modules.</p> <p>As these modules require loading of two modules to be made available which in the PE can happen in any order as the hierarchy is not build in the typical Lmod way,  adding of both the PE and custom paths to <code>MODULEPATH</code> is managed by the <code>lmodHierarchy.handshake_comnet</code> routine in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the network target module, the compiler and the MPI library.</p> <p>In the Cray PE, these are in the <code>mpi</code> subdirectory and used for the <code>cray-hdf5-parallel</code> and <code>cray-parallel-netcdf</code> modules.</p> <p>The addicion of the <code>mpi</code> subdiredctories is done in a way that is different from the other ones that depend on multiple modules, likely because the <code>cray-mpich-*</code> modules that add this level to the hierarchy themselves are at the <code>comnet</code> level and changed as soon as the network target or compiler module is changed, so that the regular Lmod approach of building a hierarchy can be used. The code that adds both the PE <code>mpi</code> directory and the corresponding custom path to the <code>MODULEPATH</code> is largely contained in  the <code>cray-mpich-*</code> modules but does use the <code>lmodHierarchy.get_user_custom_path</code> routine from <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on both the network and CPU target modules, on the compiler and      on the MPI library.</p> <p>There are currently no such examples in the PE. however, the code is prepared for  future addition of such directories in the PE itself also and the reserved directory name is <code>cncm</code>. </p> <p>As these modules require loading of two modules to be made available which in the PE can happen in any order as the hierarchy is not build in the typical Lmod way,  adding of both the PE and custom paths to <code>MODULEPATH</code> is managed by the <code>lmodHierarchy.handshake_cncm</code> routine in <code>lmodHierarchy.lua</code>.</p> </li> </ul> <pre><code>module_root\n\u251c\u2500 cpu\n\u2502  \u251c\u2500 x86-64\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u251c\u2500 x86-milan\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u251c\u2500 x86-rome\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u2514\u2500 x86-trento\n\u2502     \u2514\u2500 1.0\n\u251c\u2500 net\n\u2502  \u251c\u2500 ofi\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u2514\u2500 ucx\n\u2502     \u2514\u2500 1.0\n\u251c\u2500 compiler\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u251c\u2500 comcpu\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2502     \u251c\u2500 x86-64\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u251c\u2500 x86-milan\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u251c\u2500 x86-rome\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u2514\u2500 x86-trento\n\u2502  \u2502        \u2514\u2500 1.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u251c\u2500 comnet\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2502     \u251c\u2500 ofi\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u2514\u2500 ucx\n\u2502  \u2502        \u2514\u2500 1.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u251c\u2500 mpi\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2502     \u251c\u2500 ofi\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u2502     \u2514\u2500 cray-mpich\n\u2502  \u2502     \u2502        \u2514\u2500 8.0\n\u2502  \u2502     \u2514\u2500 ucx\n\u2502  \u2502        \u2514\u2500 1.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u2514\u2500 cncm\n   \u251c\u2500 aocc\n   \u2502  \u251c\u2500 2.2\n   \u2502  \u2514\u2500 3.0\n   \u251c\u2500 crayclang\n   \u2502  \u2514\u2500 10.0\n   \u2502     \u251c\u2500 ofi\n   \u2502     \u2502  \u2514\u2500 1.0\n   \u2502     \u2502     \u251c\u2500 x86-64\n   \u2502     \u2502     \u2502  \u2514\u2500 1.0\n   \u2502     \u2502     \u2502     \u2514\u2500 cray-mpich\n   \u2502     \u2502     \u2502        \u2514\u2500 8.0\n   \u22ee     \u22ee      \u22ee   \n   \u2502     \u2514\u2500 ucx\n   \u2502        \u2514\u2500 1.0\n   \u2514\u2500 gnu\n      \u2514\u2500 8.0\n</code></pre> <p>Given that the custom paths are not set by a single environment variable pointing to the roots of the custom installation directories and imposing the same directory structure as used by the PE, but instead by separate environment variables for every leaf in the picture below, the number of environment variables can explode.</p> <p>On LUMI, as of July 2022, we have:</p> <ul> <li>3 CPU target modules and the generic one if we want to support that also,     so 4 CPU target modules.</li> <li>1 network library (though one could argue that UCX can still be used on the      login, large memory and visualisation GPU nodes but these are not really meant     for heavy MPI work unless they are integrating with jobs on the regular compute      nodes)</li> <li>4 compiler ABIs, with one to be dropped soon (aocc/2.2, aocc/3.0, crayclang/10.0 and gnu/8.0)</li> <li>1 MPI ABI version (cray-mpich/8.0)</li> </ul> <p>This may result in:</p> <ul> <li>4 <code>LMOD_CUSTOM_CPU_*</code> environment variables</li> <li>1 <code>LMOD_CUSTOM_NET_*</code> environment variable (or 2 with UCX)</li> <li>4 <code>LMOD_CUSTOM_COMPILER_*</code> environment variables</li> <li>16 <code>LMOD_CUSTOM_COMCPU_*</code> environment variables</li> <li>4 <code>LMOD_CUSTOM_COMNET_*</code> environment variables (8 with UCX)</li> <li>4 <code>LMOD_CUSTOM_MPI_*</code> environment variables (8 with UCX)</li> <li>16 <code>LMOD_CUSTOM_CNCM_*</code> environment variables (24 with UCX as two CPU architectures don't make sense with UCX     on LUMI)</li> </ul> <p>The names of the environment variables are derived from the above directory structure, but</p> <ul> <li>All characters uppercase</li> <li><code>/</code>, <code>-</code>and <code>.</code> get substituted by an underscore character</li> <li>The resulting name is prefixed with <code>LMOD_CUSTOM_</code> and postfixed with <code>_PREFIX</code>.</li> </ul>"},{"location":"03_Slurm/","title":"Slurm on LUMI","text":"<ul> <li>Doc links</li> <li>GPU binding</li> <li>Slurm in a container</li> <li>Miscellaneous tips&amp;tricks</li> </ul>"},{"location":"03_Slurm/03_01_Doclinks/","title":"Slurm documentation links","text":"<p>Note: Check the version of Slurm after each LUMI update.</p> <p>The version after the October-November 2023 update is 22.05.10.</p>"},{"location":"03_Slurm/03_01_Doclinks/#web-documentation","title":"Web documentation","text":"<ul> <li> <p>Slurm version 22.05.10, on the system at the time of the course</p> </li> <li> <p>Slurm manual pages are also all on the web      and are easily found by Google, but are usually those for the latest version.</p> <ul> <li> <p><code>man sbatch</code></p> </li> <li> <p><code>man srun</code></p> </li> <li> <p><code>man salloc</code></p> </li> <li> <p><code>man squeue</code></p> </li> <li> <p><code>man scancel</code></p> </li> <li> <p><code>man sinfo</code></p> </li> <li> <p><code>man sstat</code></p> </li> <li> <p><code>man sacct</code></p> </li> <li> <p><code>man scontrol</code></p> </li> </ul> </li> </ul>"},{"location":"03_Slurm/03_02_GPU_binding/","title":"GPU binding issues","text":""},{"location":"03_Slurm/03_02_GPU_binding/#nature-of-the-problem","title":"Nature of the problem","text":"<p>When using per-task cgroups, P2P IPC doesn't work as a communication mechanism as GPUs  cannot find each other. It is used, e.g., by Cray MPICH for all but very short messages  when communicating directly between GPUs in a node.</p> <p>See also the schedMD case #17875.</p>"},{"location":"03_Slurm/03_02_GPU_binding/#setups-that-work","title":"Setups that work","text":"<p>TODO</p>"},{"location":"03_Slurm/03_03_Slurm_in_container/","title":"How to get Slurm into a container?","text":"<p>According to Mihkel:</p> <ul> <li> <p>Basic use: </p> <pre><code>SINGULARITY_BIND=\"$SINGULARITY_BIND,/usr/lib64/slurm/,/etc/slurm,/etc/passwd,/usr/lib64/libmunge.so.2,/run/munge,/var/lib/misc,/etc/nsswitch.conf\"\n</code></pre> </li> <li> <p>More advanced, including all the executables:</p> <pre><code>export SINGULARITY_BIND=\"$SINGULARITY_BIND,/usr/bin/sacct,/usr/bin/sacctmgr,/usr/bin/salloc,/usr/bin/sattach,/usr/bin/sbatch,/usr/bin/sbcast,/usr/bin/scancel,/usr/bin/scontrol,/usr/bin/scrontab,/usr/bin/sdiag,/usr/bin/sinfo,/usr/bin/sprio,/usr/bin/squeue,/usr/bin/sreport,/usr/bin/srun,/usr/bin/sshare,/usr/bin/sstat,/usr/bin/strigger,/usr/bin/sview,/usr/bin/sgather,/usr/lib64/slurm/,/etc/slurm,/etc/passwd,/usr/lib64/libmunge.so.2,/run/munge,/var/lib/misc,/etc/nsswitch.conf\"\n</code></pre> </li> </ul>"},{"location":"03_Slurm/03_99_misc/","title":"Miscellaneous Slurm tips&amp;trics","text":""},{"location":"03_Slurm/03_99_misc/#system-configuration","title":"System configuration","text":"<ul> <li> <p>List of nodes with their Slurm features:     <pre><code>sinfo -N -o \"%N %f\"\n</code></pre></p> </li> <li> <p>Extensive information about the partitions:     <pre><code>scontrol show partition --all\n</code></pre>     With `--all`` to also see hidden partitions.</p> </li> <li> <p>Show topology:     <pre><code>scontrol show topology\n</code></pre></p> </li> <li> <p>Show extensive configuration info:     <pre><code>scontrol show config\n</code></pre></p> </li> <li> <p>Also interesting:     <pre><code>scontrol show assoc_mgr\n</code></pre></p> </li> </ul>"},{"location":"03_Slurm/03_99_misc/#user-or-project-related","title":"User- or project-related","text":"<ul> <li> <p>Show which partitions a project has access to:     <pre><code>sacctmgr show assoc where account=project_462000087\n</code></pre></p> </li> <li> <p>Show which partitions a user has access to:     <pre><code>sacctmgr show assoc format=account,Partition  -p Users=kurtlust\n</code></pre></p> </li> </ul>"},{"location":"03_Slurm/03_99_misc/#job-management","title":"Job management","text":"<ul> <li>History of slurm jobs:<ul> <li>Through <code>sacct</code> <pre><code>sacct -a\n</code></pre></li> <li>Through <code>slurm</code> <pre><code>slurm history 2hours -a \n</code></pre>     or from a particular user:     <pre><code>slurm history 1day -u &lt;username&gt;\n</code></pre></li> </ul> </li> </ul>"},{"location":"03_Slurm/03_99_misc/#reports","title":"Reports","text":"<ul> <li> <p>Report for a project, usage per user:</p> <pre><code>sreport cluster AccountUtilizationByUser account=project_465001098\n</code></pre> </li> </ul>"},{"location":"03_Slurm/03_99_misc/#misc","title":"Misc","text":"<ul> <li> <p>Attach to a running job rather than ssh into a node:     <pre><code>srun --pty --jobid &lt;jobid&gt; bash\n</code></pre>     or     <pre><code>srun --pty --jobid &lt;jobid&gt; --w &lt;nodename&gt; bash\n</code></pre></p> </li> <li> <p>Testing on LUMI-D:     <pre><code>srun -plumid -t30:00 --gres=gpu:1 --pty bash\n</code></pre></p> </li> </ul>"},{"location":"04_AI_packages/","title":"AI software on LUMI","text":"<ul> <li>TensorFlow</li> </ul>"},{"location":"04_AI_packages/04_02_TensorFlow/","title":"TensorFlow","text":""},{"location":"04_AI_packages/04_02_TensorFlow/#tensorflow-profiling","title":"TensorFlow profiling","text":"<p>Based on information from Orian.</p> <p>TensorFlow has a built-in profiler and this is probably the first thing to try in case of performance problems with TensorFlow, rather than using the AMD profiling tools. TensorBoard can then be used to analyse the profiler data.</p> <p>There is an article \"TensorFlow Profiler in practice: Optinmizing TensorFlow models on AMD GPUs\" in the AMD ROCm blogs.</p> <p>Note: Orian once said that the root profile log directory should contain a profile file or TensorBoard will not detect any profile and not show the profile tab. The trick is to move one of the profile logs to that directory.</p>"},{"location":"05_MPI/","title":"MPI","text":"<ul> <li> <p>Cray MPICH tips&amp;tricks</p> </li> <li> <p>Open MPI</p> </li> <li> <p>OSU micro-benchmarks</p> </li> <li> <p>Open MPI tips&amp;tricks</p> </li> <li> <p>Misc</p> </li> </ul>"},{"location":"05_MPI/05_01_Cray_MPICH_tips/","title":"Cray MPICH tips","text":""},{"location":"05_MPI/05_01_Cray_MPICH_tips/#remarks-from-the-november-2023-hackathon","title":"Remarks from the November 2023 hackathon","text":"<ul> <li> <p>Using hugepages can improve intra-node performance (or performance overall?)</p> </li> <li> <p>Intra-node performance can improve by setting</p> <pre><code>export MPICH_SMP_SINGLE_COPY_MODE=NONE\n</code></pre> <p>at least for large message sizes.</p> <p>(Partly) documented in the <code>intro_mpi</code> man page.</p> </li> </ul>"},{"location":"05_MPI/05_01_Cray_MPICH_tips/#ofi-error-messages-and-potential-workarounds","title":"OFI error messages and potential workarounds","text":"<ul> <li> <p><code>PLTE_NOT_FOUND</code></p> <p>Error message:</p> <pre><code>MPIDI_OFI_handle_cq_error(1062): OFI poll failed (ofi_events.c:1064:MPIDI_OFI_handle_cq_error:Input/output error - PTLTE_NOT_FOUND)\n</code></pre> <p>HPE Cray recommendation:</p> <pre><code>export FI_CXI_MSG_OFFLOAD=0\nexport FI_CXI_REQ_BUF_COUNT=200\n</code></pre> <p>These are undocumented environment variables.</p> </li> <li> <p><code>UNDELIVERABLE</code></p> <p>Error message:</p> <pre><code>MPIDI_OFI_handle_cq_error(1067): OFI poll failed (ofi_events.c:1069:MPIDI_OFI_handle_cq_error:Input/output error - UNDELIVERABLE)\n</code></pre> <p>One possible cause is a broken switch so that messages can no longer be delivered.</p> <ul> <li>Symptoms on the GPU nodes: A pattern of a number of subsequent either even-numbered or odd-numbered nodes.</li> </ul> </li> </ul>"},{"location":"05_MPI/05_01_Cray_MPICH_tips/#other-mpi-error-messages","title":"Other MPI error messages","text":"<ul> <li> <p><code>xpmem_attache failed on</code></p> <p>Error message:</p> <pre><code>MPIDI_CRAY_XPMEM_do_attach(482)...........: xpmem_attach failed on rank 5 (src_rank 20, vaddr 0x3abf5b20, len 2400)\n</code></pre> <p>It is a memory registration cache bug. The default MR monitor is known to be buggy. The memory registration cache keeps track of what memory regions have been pinned and mapped to the network interface. A bug in the registration cache monitor can have some unexpected side effects, like memory regions deallocated by the code might not be correctly unregistered, which can result in future buffers that share the same virtual addresses being mapped incorrectly or thrown back. Steve Abbott (HPE Frontier support) thinks memhooks is better but hasn't gotten much testing yet, and long term (towards the end of 2023) HPE intends to implement a solution that makes it irrelevant and is better all around.</p> <p>Solutions:</p> <ul> <li> <p>Preferred: </p> <pre><code>export FI_MR_CACHE_MONITOR=memhooks\n</code></pre> <p>to use the memhooks memory monitor.</p> <p>Undocumented (or has this become irrelevant in the current versions)?</p> </li> <li> <p>Alternative:</p> <pre><code>export FI_MR_CACHE_MAX_COUNT=0\n</code></pre> <p>which turns off the memory registration cache all together but mibht be pretty punishing on performance (code-dependent).</p> <p>Documented in the <code>intro_mpi</code> man page.</p> </li> </ul> </li> </ul>"},{"location":"05_MPI/05_01_Cray_MPICH_tips/#investigating-crashes","title":"Investigating crashes","text":"<p>Based on tips from Juha Jaykka.</p> <ul> <li> <p>Use <code>sacct</code> to figure out what nodes a job is using and in what time interval it ran:</p> <pre><code>sacct --job=$jobid --format=JobID,JobName,State,ExitCode,Start,End,NodeList%1000\n</code></pre> </li> <li> <p>Now use <code>sacctmgr</code> to list events on those nodes during the time window. You can      copy-paste date/times and the nodelist from the previous command:</p> <pre><code>sacctmgr list event Start=&lt;starttime&gt; End=&lt;endtimne&gt; Nodes=&lt;nodelist&gt; format=NodeName,TimeStart,Reason%150\n</code></pre> </li> <li> <p>Sometimes you can get more information about a specific node with a fault via <code>scontrol show node</code>.      So for a faulty node from the previous command, you can run:</p> <pre><code>scontrol show node &lt;nodename or nodelist&gt;\n</code></pre> </li> </ul> <p>Note that after many node failures, a node goes automatically into drain mode, so it does not make sense to manually exclude them in your job scripts.</p>"},{"location":"05_MPI/05_02_OpenMPI/","title":"Open MPI on LUMI","text":""},{"location":"05_MPI/05_02_OpenMPI/#build-instructions-alfio-from-the-hackathon","title":"Build instructions Alfio from the hackathon","text":"<pre><code>module load PrgEnv-gnu\nmodule unload cray-mpich\nOMPI_VERSION=4.1.6\necho \"Install OpenMPI v\"$OMPI_VERSION\nwget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-$OMPI_VERSION.tar.bz2\ntar xf openmpi-$OMPI_VERSION.tar.bz2 &amp;&amp; rm openmpi-$OMPI_VERSION.tar.bz2\ncd openmpi-$OMPI_VERSION\n\n# SLURM (default yes), OFI, PMI\nmake clean\n./configure --prefix=$PWD/install_ofi_slurm_pmi/ \\\n--with-ofi=/opt/cray/libfabric/1.15.2.0 --with-slurm --with-pmi=/usr CC=gcc CXX=g++ FTN=gfortran CFLAGS=\"-march=znver2\" \\\n--disable-shared --enable-static\nmake -j 10 install\n\nrm -f install\nln -s $PWD/install_ofi_slurm_pmi install\n\nmodule unload cray-mpich\nexport PATH=$PWD/install/bin/:$PATH\nexport LD_LIBRARY_PATH=$PWD/install/lib:$LD_LIBRARY_PATH\n\n# Test with OSU\nwget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-5.9.tar.gz\ntar xf osu-micro-benchmarks-5.9.tar.gz\nrm osu-micro-benchmarks-5.9.tar.gz\ncd osu-micro-benchmarks-5.9\n\n./configure --prefix=$PWD/install CC=$(which mpicc) CXX=$(which mpicxx) CFLAGS=-O3 CXXFLAGS=-O3\nmake -j 10 install\n\ncd install/libexec/osu-micro-benchmarks/mpi/p2p \nsrun -n 2 --ntasks-per-node=1 ./osu_bw\n\n# Cannot remember the performance, try to compare to cray-mpich\n</code></pre> <p>It looks like you can now safely build shared libraries though.</p> <p>Useful links:</p> <ul> <li>Use at NERSC</li> </ul>"},{"location":"05_MPI/05_02_OpenMPI/#open-mpi-5-for-slingshot-11","title":"Open MPI 5 for Slingshot 11","text":"<p>Useful links:</p> <ul> <li> <p>CUG paper ORNL</p> <p>The paper says that changes needed in Open MPI itself are present in the Open MPI main and v5.0.x branches, but not in branches for older versions.</p> <p>One problem with Open MPI on OFI+CXI was that the CXI provider has no special pathway for intra-node messaging. Rather than redeveloping the MTL to deal with multiple components concurrently, a new provider was made, LINKx, encapsulating CXI but adding the missing features.</p> </li> <li> <p>Use at NERSC</p> </li> </ul>"},{"location":"05_MPI/05_02_OpenMPI/#open-mpi-with-spack","title":"Open MPI with Spack","text":"<p>Spack needs to be told how to use the proper libfabric library.</p> <p>A user gave some information in ticket #3228 on how they did it:</p> <ul> <li> <p>Add to the spack.yaml file:</p> <pre><code>packages:\n  libfabric:\n    externals:\n      - spec: libfabric@1.15.2.0\n        prefix: /opt/cray/libfabric/1.15.2.0\n</code></pre> <p>I guess this could go in <code>/etc/spack/packages.yaml</code> in the Spack installation itself.</p> </li> <li> <p>When calling Spack to build Open MPI, specify <code>^libfabric@1.15.2.0</code>.</p> </li> </ul>"},{"location":"05_MPI/05_03_OSU_benchmarks/","title":"OSU micro-benchmarks","text":"<ul> <li> <p>Home page of the benchmarks</p> </li> <li> <p>Examples of running the benchmarks:</p> <ul> <li> <p>University of Luxemburg HPC tutorial page</p> </li> <li> <p>AWS HPC workshop</p> </li> <li> <p>Sigma2 page</p> </li> </ul> </li> </ul>"},{"location":"05_MPI/05_04_OpenMPI/","title":"Open MPI tips&amp;tricks","text":"<p>From a user Tor worked with: The following environment variables help to run on multiple nodes:</p> <pre><code>#Required to launch softwares with mpirun and HPE CXI provider\nexport OMPI_MCA_ras_base_launch_orted_on_hn=1\n\n#To avoid problem of sending too large message\nexport OMPI_MCA_coll_tuned_use_dynamic_rules=1\nexport OMPI_MCA_coll_tuned_allreduce_algorithm=5\nexport OMPI_MCA_coll_tuned_allreduce_algorithm_segmentsize=131072\n\n#To avoid problem of sending too large message\nexport OMPI_MCA_coll_tuned_bcast_algorithm=4 # or 9\nexport OMPI_MCA_coll_tuned_bcast_segment_size=131072\n</code></pre>"},{"location":"05_MPI/05_99_misc/","title":"Miscellaneous topics","text":""},{"location":"05_MPI/05_99_misc/#cray-mpich-versions","title":"Cray MPICH versions","text":"CPE cray-mpich 21.08 8.1.8 21.12 8.1.12 22.08 8.1.18 22.12 8.1.23 23.03 8.1.25 23.09 8.1.27 23.12 8.1.28 24.03 8.1.29"},{"location":"05_MPI/05_99_misc/#cxi-went-public","title":"CXI went public","text":"<p>At the end of January 2024, CXI was upstreamed to the  libfabric GitHub repository in the libfabric/prov/cxi subdirectory, so it is now possible to build libfabric ourselves (at least once the bugs are fixed, on February 9 2024 Alfio mentioned that there are still problems with missing header files etc.)</p>"},{"location":"05_MPI/05_99_misc/#workarounds","title":"Workarounds","text":"<ul> <li> <p>Containers from before the system update that are built with ROCm 5.x and that import the     MPI libraries from the system, may produce an error afterwards about not being able to     find <code>libamdhip64.so.6</code>. This is because the GTL library in the MPI libraries from 24.03     is built for the ROCm 6 runtime.</p> <p>Workaround is to try to make the code link with the GTL library from MPICH 8.1.27: <pre><code>export LD_LIBRARY_PATH=/opt/cray/pe/mpich/8.1.27/gtl/lib/:$LD_LIBRARY_PATH\n</code></pre> in the container.</p> </li> </ul>"},{"location":"06_ROCm/","title":"ROCm","text":"<ul> <li>Miscellaneous</li> </ul>"},{"location":"06_ROCm/06_99_misc/","title":"ROCm miscellaneous issues","text":""},{"location":"06_ROCm/06_99_misc/#driver-version-compatibility","title":"Driver-version compatibility","text":""},{"location":"06_ROCm/06_99_misc/#523-driver-version","title":"5.2.3 driver version","text":"<ul> <li> <p>ROCm 5.4.3 appears to work</p> </li> <li> <p>ROCm 5.6.1: </p> <ul> <li>According to Samuel there are some known issues with queries     about available memory being off but it hasn't been a blocker for apps.</li> <li>Peter mentioned that he couldn't get Neko to work with ROCm 5.,6, with a failure in     <code>hipDeviceGetStreamPriorityRange</code> right at the start.</li> <li>The performance of GPU-aware MPI can be very bad in combination with the Cray MPICH      modules on the system in early 2024.</li> </ul> </li> </ul>"},{"location":"06_ROCm/06_99_misc/#rocfft-crashes","title":"rocFFT crashes","text":"<p>In some cases, crashes in rocFFT are due to cache overwrite. One workaround that worked in DestinE is  using <pre><code>export ROCFFT_RTC_CACHE_PATH=/dev/null\n</code></pre> or <pre><code>export ROCFFT_RTC_CACHE_PATH=/tmp\n</code></pre></p>"},{"location":"07_Lustre/","title":"Lustre","text":"<ul> <li>Miscellaneous</li> </ul>"},{"location":"07_Lustre/07_99_misc/","title":"Miscellaneous topics: Lustre","text":"<ul> <li>Tips for speeding up modules (from Jean-Yves Vet, HPE)<ul> <li>DNE2/3 on the directories containing the modules (I don't believe this is enabled as      most of the    metadata points to a single MDS).</li> <li>Write those small files as DoM (data over metadata) to avoid the OSS round trip</li> </ul> </li> </ul>"},{"location":"08_LUMI-O/","title":"LUMI-O","text":"<ul> <li>Miscellaneous topics: LUMI-O</li> </ul>"},{"location":"08_LUMI-O/08_99_misc/","title":"Miscellaneous topics: LUMI-O","text":"<p>From the sysadmins:</p> <p>LUMI-O and the HPC side share only datacenter floorspace, power and upstream network connectivity.    They really are separate systems, also in terms of scheduled maintenance and downtime.    So while the HPC system is unavailable the object storage can be used to move their data around.</p> <p>LUMI-O is managed by a different team, the Open Cloud team of CSC.</p>"},{"location":"09_Software/","title":"Remarks about individual software packages","text":"<ul> <li> <p>GROMACS</p> </li> <li> <p>ParaView</p> </li> <li> <p>Python</p> </li> <li> <p>R</p> </li> </ul>"},{"location":"09_Software/09_G01_GROMACS/","title":"GROMACS","text":"<ul> <li>Interesting link about a <code>hipMemsetAsync()</code> error message.     The discussion has input from one of the developers.</li> </ul>"},{"location":"09_Software/09_P01_ParaView/","title":"ParaView","text":""},{"location":"09_Software/09_P01_ParaView/#running-the-ood-container-by-hand-as-it-is-broken-in-ood-end-of-2024","title":"Running the OOD container by hand as it is broken in OOD (end of 2024)","text":"<p>Instructions provided by Orian.</p> <p>The <code>getEglCard</code> script in the container in use at the end of 2024 provides the wrong GPU device. It sets <code>VGL_DISPLAY</code> to <code>/dev/dri/card0</code> which does not correspond to what <code>nvidia-smi</code> suggests:</p> <pre><code>/dev/dri/by-path/pci-0000:48:00.0-card -&gt; ../card1\n</code></pre> <p>The solution seems to be to run in a desktop session and then</p> <pre><code>singularity exec --nv /appl/local/ood/$SLURM_OOD_ENV/container_images/paraview_5.11.sif \\\n  bash -c 'export VGL_DISPLAY=/dev/dri/card1; vglrun paraview'\n</code></pre> <p>Check the \"About\" dialog in ParaView to confirm that the GPU is being used. It should show a proper OpenGL version and as the OpenGL renderer \"NVIDIA A40/PCIe/SSE2\". </p>"},{"location":"09_Software/09_P02_Python/","title":"Python","text":"<p>Cray Python was built with the GNU compilers.</p>"},{"location":"09_Software/09_P02_Python/#mpi4py","title":"mpi4py","text":"<ul> <li> <p>Error messages about <code>PMPI_Mprobe</code>: Early versions of the Cassini provider don't support the     MPI <code>MProbe</code> functions. The workaround is to set</p> <pre><code>export MPI4PY_RC_RECV_MPROBE='False'\n</code></pre> </li> <li> <p>mpi4py complains with</p> <pre><code>Attempting to use an MPI routine before initializing MPICH\n</code></pre> <p>According to the course materials it could be related to mpi4py being built with GCC, and the workaround is to use</p> <pre><code>LD_PRELOAD=/opt/cray/pe/lib64/libmpi_gnu_91.so\n</code></pre> </li> </ul>"},{"location":"09_Software/09_R01_R/","title":"R tips and tricks","text":""},{"location":"09_Software/09_R01_R/#misc","title":"Misc","text":"<ul> <li> <p>Some packages are affected because <code>timedatectl</code> doesn't work for regular LUMI users     (with the error message <code>Failed to query server: Endpoint is not connected</code>).</p> <p>Note that you get the same error message when you actually try to run <code>sysctl</code>.</p> <p>Workaround: Add the <code>TZ</code> environment variable to <code>.Renviron</code> as, e.g., <code>TZ=\u201cEurope/Berlin\u201d</code></p> </li> </ul>"},{"location":"98_Linux/","title":"Rather more general Linux tips","text":"<ul> <li> <p>File management</p> </li> <li> <p>Bash tips</p> </li> </ul>"},{"location":"98_Linux/98_01_File_management/","title":"File management tips and tricks","text":"<ul> <li> <p>How to quickly check the number of files?</p> <ul> <li> <p>Solution with <code>rsync</code>:</p> <pre><code>rsync --stats --dry-run\n</code></pre> </li> <li> <p>Solution with <code>lfs find</code> (but will count directories also):</p> <pre><code>lfs find . | wc -l\n</code></pre> </li> </ul> </li> <li> <p>Erasing a lot of files at once (Pawsey tip):     [\"Deleting Large Number ]of Files onm Lustre Filesystems](https://pawsey.atlassian.net/wiki/spaces/US/pages/51925900/Deleting+Large+Numbers+of+Files+on+Lustre+Filesystems).</p> <p>Basically, run</p> <pre><code>find -P ./processor0 -type f -print0 -o -type l -print0 | xargs -0 munlink\nfind -P ./processor0 -type d -empty -delete\n</code></pre> <ul> <li> <p><code>-P</code>: This option restricts the search within the indicated directory tree and forces NO dereference of symbolic links. This warranties that the find command will not look for files within the links.</p> </li> <li> <p><code>./processor0</code>: This argument is the directory on which the search (and deletion) will be performed.</p> </li> <li> <p><code>-type f -o -type l</code>: These options indicate that the find command will search for anything that is a file (-type f) or (-o) a soft link (-type l, this is the lower letter l). As indicated with the -P option above, the links are not followed, so only the links will be removed but the object to where they linked are not.</p> </li> <li> <p><code>-print0</code>: This option indicates the format of the result of the \"find\" command. This particular format is able to catch strange file names, and ensures that they are readable for the following command  (xargs) which has been concatenated with the pipe. Note that two -print0 indications are needed, one per \"side\" of the \"or\" option indicated above.</p> </li> <li> <p>The pipe command ( represented by a single pipe line: |)     This command concatenates two commands. This makes the output of the previous command (find) to serve as input to the following command (xargs).</p> </li> <li> <p><code>xargs -0</code>: xargs will then convert the received list of files, line by line, into an argument      for whatever command is specified to it (munlink in this case). The -0 flag is related to the      format of the listed files; if you use -print0 in the find command you must use -0 in the xargs command.</p> </li> <li> <p><code>munlink</code>: This command deletes each file and soft link in the list without overloading the metadata server. In this case, the list is the one received by xargs.</p> </li> <li> <p>The second find command will search the directory processor0 and all subdirectories for any empty directories (-type d -empty) and delete them. </p> </li> </ul> </li> </ul>"},{"location":"98_Linux/98_02_Bash_tips/","title":"Bash tips","text":"<ul> <li> <p>Global substitution in the previous command:</p> <pre><code>!!:gs/GNU/Cray/\n</code></pre> </li> </ul>"},{"location":"99_Misc/","title":"Miscelaneous topics","text":"<ul> <li> <p>LUMI flop computation</p> </li> <li> <p>Login node organisation</p> </li> <li> <p>HPE software releases</p> </li> <li> <p>EESSI</p> </li> <li> <p>Energy consumption</p> </li> <li> <p>Answers to frequent user complaints</p> </li> </ul>"},{"location":"99_Misc/99_01_Flops/","title":"LUMI peak performance","text":"<p>Thanks to Orian for some of the data.</p>"},{"location":"99_Misc/99_01_Flops/#top-500-result","title":"Top-500 result","text":"<p>Preliminaries</p> <ul> <li>The GPU clock for the computations is set to 1600 MHz, the same as the memory clock,     instead of the theoretical maximum of 1700 MHz. It is also what is in practice used     on LUMI?</li> <li>The peak flop computation is based on the vector units rather than the matrix units.</li> </ul> <p>Computations:</p> <ul> <li>RPeak of 1 GPU = 220 CUs * 128 FMA Flop/CU/clock * 1600 MHz = 45.056 TFlops</li> <li>RPeak of 1 CPU of LUMI-G = 64 core * 16 FMA Flop/core/clock * 2.0 GHz = 2.048 TFlops</li> <li>RPeak of 1 LUMI-G node = 4 * 45.056 TFlops + 2.048 TFlops = 182.272 TFlops</li> </ul> <p>The result in the November 2023 and June 2024 Top500 listing was obtained on 2916 nodes of LUMI-G.</p> <ul> <li>Computation of RPeak: 2916 * ( 4 * 220 CUs * 128 FMA Flop/CU/clock * 1.600 GHz <ul> <li>64 core * 16 FMA Flop/core/clock * 2.0 GHz) = 531.51 PFlops</li> </ul> </li> <li>For the computation of the number of cores, each CU was considers a core, and each     CPU core, so the total number is     2916 * (4 * 220 + 64) = 2752704</li> </ul>"},{"location":"99_Misc/99_01_Flops/#single-node-hpl","title":"Single node HPL","text":"<ul> <li>1 x MI250x (2 GCDs) = 40.45 TFlops (21.07 on 1 GCD)</li> <li>4 x MI250x (8 GCDs) = 161.97 TFlops</li> </ul> <p>(which is 23% better than the per-node performance in the Top500 result)</p>"},{"location":"99_Misc/99_02_LoginNodes/","title":"Misc remarks for the login nodes","text":"<p>Message Ville on May 15, 2024:</p> <p>We are also experimenting with a cgroup based CPU usage limit which is now in place on uan01.  It will limit the number of cores a user can consume on a single ssh session to 16 cores.  If this works well, we will put this into use on the other uan's as well.  We started also looking into limiting the memory usage and enabling the use of NVME as  /tmp and /var/tmp backing storage. We may have a path forward with that using  private namespaced /tmp mounts for each user. We could enable this later  on the LUST uan node if you'd like to test and see it action.</p> <p>We can also have a separate directory on the nvme for that too. We would just mount the NVME  under /local and create /local/tmp with mode 777 and then have the namespaced tmp mounts  backed by directory like /local/ns/tmp and /local/ns/var-tmp. This way we would not take away  any use case (although the path would change) and still get the benefits. I have this exact  setup now on one of the uan's (which is not user or lust accessible) made by hand and I think  it looks pretty nice. If you like we can change uan06 to have the same setup tomorrow for  you to check out. This is not yet in the configuration management, so the changes will  be lost at boot, but we can enable it on the fly for uan06 and new sessions will get  the private tmp. If it is good we could start the process to get that change done for uan[01-04] too.</p> <p>To see the current limits:</p> <pre><code>$ cat /etc/systemd/system/user-.slice.d/50-userlimits.conf\n[Slice]\nCPUQuota=1600%\nMemoryMax=96G\n</code></pre> <p>We have a private tmp:</p> <pre><code>$ cat /etc/security/namespace.conf\n/tmp     /local/ns/tmp/         level      root\n/var/tmp /local/ns/var-tmp/     level      root\n</code></pre>"},{"location":"99_Misc/99_03_SystemEnvironments/","title":"System environments","text":""},{"location":"99_Misc/99_03_SystemEnvironments/#hpe-releases","title":"HPE Releases","text":"<ul> <li>May 2024<ul> <li>CSM 1.5.1</li> <li>SHS 2.2</li> <li>COS 23.1.1.3 / SLES 15 SP5</li> <li>CPE 24.3</li> </ul> </li> <li>July 2024 releases:<ul> <li>CSM 1.5.2</li> <li>SHS 11.0.0</li> <li>COS 24.7 / SLES 15 SP5</li> <li>CPE 24.7</li> <li>AMD ROCm 6.1</li> </ul> </li> </ul>"},{"location":"99_Misc/99_04_EESSI/","title":"EESSI and LUMI","text":""},{"location":"99_Misc/99_04_EESSI/#some-background","title":"Some background","text":"<p>The only currently supported way of distributing EESSI is via CernVM-FS, a system based on web technology developed to distribute the software for the LHC experiments.</p> <p>It was later picked up by the organisation formerly known as Compute Canada, now part of the Digital Research Allicance of Canada (called The Alliance further in this text, as that is what they also often do), as a way to manage a central software stack that is available on all their HPC clusters that are geographically spread over Canada, but managed in a central (so people working on that stack also have access to all of these clusters and their say in the specifications when the clusters are acquired,  which becomes important later in the discussion).</p> <p>To make this cross-cluster installation feasible, they had to abstract several aspects of the OS also, as different versions of Linux could require different settings for software packages build on top of it. So they first install a set of OS libraries as a basis. Currently they use  Gentoo Prefix for that as this enables installation in a separate set of directories (and hence in the CernVM-FS-served tree and seemingly without coming in  conflict with the vendor-provided OS and cluster management tools).</p> <p>This setup served as the basis for the EESSI architecture.</p> <p>Though EESSI builds on the architecture used by The Alliance, it is not completely the same. E.g., both projects had to deal with not being allowed to spread certain libraries via CernVM-FS. The primary example are the NVIDIA libraries. The Alliance assumes that they are installed in a certain location while  EESSI assumes that there is a subdirectory <code>/opt/EESSI</code> which will be used for a number of site-specific adaptations that may be needed, including the injection of some CUDA libraries that may not be distributed by them for sites with NVIDIA GPUs.</p> <p>Another difference is the way they use EasyBuild. EESSI is on paper a separate project from EasyBuild, but it is really mostly run by a subset of people running EasyBuild and it follows very closely  EasyBuild. The Alliance however, hasn't been shy in the past of going for  some of their own EasyConfigs](https://github.com/ComputeCanada/easybuild-easyconfigs)  to solve problems specific to their users and sites, even though they do contribute back and participate actively in the development of EasyBuild. (Notice that the linked repository, though a clone of the central EasyConfigs repository, has its own main branch that can be a lot of commits ahead from even the developers branch of the main EasyBuild EasyConfigs repository)</p> <p>What they have in common is that they also distribute their stack to other interested parties. For The Alliance, this is documented in their documentation, for EESSI this is inherent to their project as they have no control or access to even the  majority of the clusters that use their software stack.</p> <p>Links</p> <ul> <li>Digital Research Alliance of Canada documentation</li> </ul>"},{"location":"99_Misc/99_04_EESSI/#why-is-eessi-not-a-very-good-idea-on-lumi","title":"Why is EESSI not a very good idea on LUMI?","text":""},{"location":"99_Misc/99_04_EESSI/#cernvm-fs-on-a-cray-system","title":"CernVM-FS on a Cray system.","text":"<p>CernVM-FS is a remote file system based on web technology. A textbook setup requires:</p> <ol> <li>A daemon running on each compute node providing CernVM-FS (which requires the <code>/cvmfs</code> mount point),</li> <li>Cache space for each CernVM-FS daemon, and</li> <li>A web cache, certainly for larger sites.</li> </ol> <p>However, Cray OS on the LUMI compute nodes starts from a very different idea. Though it is a derivative of SUSE Enterprise Linux 15 (now SP4, after the planned system upgrade of August 2024 SP5), it has many  daemons disabled to reduce background noise from the OS, also known as OS jitter, as this restricts  scalability of very large parallel applications. In fact, even with those measures, during the acceptance testing of the GPU partition on LUMI, scaling of some codes did not meet the acceptance criteria, leading to the necessity to make one core unavailable to users to reserve it for OS processes. (In essence, the AMD driver stack was causing too much OS jitter.) As part of the measures to control OS jitter, HPE Cray also offers only one remote file system on the compute nodes, Lustre, while all other remote file systems are offered through a system they call DVS or Data Virtualisation Service, which essentially forwards all file system calls to management nodes that run  the actual client (a bit like a remote procedure call).  </p> <p>Note that the documentation of CernVM-FS does mention that some sites have experimented with solutions to  make CernVM-FS available on the compute nodes in other ways but the documentation itself mentions that it is not recommended for performance and in fact also mentions another difficulty (with inodes) for which there does seem to be a workaround.</p> <p>Both paths are not an option on LUMI. It is clear that adding a permanently enabled daemon on the compute nodes, and one that is not supported by our vendor, is not an option. It endangers the scalability and it would slow down every system update process on LUMI as LUMI would first have to be updated by the vendor to a vendor-supported configuration after which the additional daemon would have to be reinstalled and integrated in the management environment. Our compute nodes are diskless, which is not optimal for the caching requirements of CernVM-FS. There is a scheme though (via a loopback  filesystem) to do that on shared storage in a metadata server-friendly way, which is not optimal for performance, but the load on the shared file system would not be worse than with traditionally installed software. Lastly, a number of Squid proxies and preferably a Stratum-1 server would be needed to keep the load on the network connection to the internet low and to not put  an unacceptable load on whatever external server the software is fetched from.  A presentation by one of the developers at the 6th EasyBuild User Meeting  advised one server per 500 nodes, though that again is a number from 4 years ago. Still, if the goal of the development project is to make EESSI available on all of LUMI to offer the software from the MultiXScale project, it requires a considerable amount of hardware for which there is no budget in the  LUMI project.</p> <p>The second path is not an option either. Firstly, it is high risk as the CernVM-FS documentation recommends against it and as experience has learned that getting something to work with DVS is non-trivial. Secondly, it also requires a considerable investment in hardware as the necessary management nodes are needed to host the CernVM-FS instances to which DVS will forward the requests. One could likely do with far less cache servers though as in this scenario each CernVM-FS instance would likely be serving multiple nodes, reducing the number of instances. It is likely a stratum-1 server (or two in some kind of high reliability setup) would be enough. However, as now we need CernVM-FS in the management domain, it still interferes with system updates. </p> <p>Both scenarios also require a significant time investment from the sysadmins, both for the first installation (where the second option, as it is more experimental, may be even worse) and to maintain the installation and keep it secure. Even if we would have the necessary hardware and people resources to do the work, it is not something that could be done on the fly on a system  like LUMI, but something that needs to be carefully tested before deployment, and that deployment can only be done during a system maintenance interval. As the testing can only be conclusive when done at scale, it is something that is not really feasible now that the pilot projects have ended, as it would confront users with a potentially unstable or underperforming system during testing and as  considerable capacity would be needed to put a realistic load on CernVM-FS.</p> <p>Note also that SUSE Linux is only a B-platform for CernVM-FS (at least in the most recent information we could find on this), so the question is whether you even want to run this with that level on support on a machine like LUMI where each day of downtime is worth probably 150,000 EURO or more.</p> <p>It is also possible to run CernVM-FS through a tool called <code>cvmfsexec</code>. However, this tool is also of no use on LUMI as (1) user name spaces are disabled due to security concerns and (2) fuse mounts are not  available either, with no planned date for future availability as they will not be made available until a robust clean-up script is developed to clean up a node when a job ends.</p> <p>Technically speaking, it is possible to run CernVM-FS inside a singularity container on LUMI. However, doing so at scale without proper caching infrastructure (as LUMI cannot offer that) has the potential to create an immense network load  on both the outgoing network connections of LUMI and on the stratum-1 server that would be used, which in turn may lead to complaints from the admins of that server to CSC. Both are not desirable, and using containers at scale as a backdoor will be considered abuse of the LUMI infrastructure.</p> <p>There are enough fine ways of distributing software that are a better match with LUMI and requiring less effort from sysadmins that CernVM-FS for software distribution should not be a priority for any future development.</p>"},{"location":"99_Misc/99_04_EESSI/#eessi-software-on-lumi","title":"EESSI software on LUMI","text":"<p>EESSI is currently not fully compatible with LUMI.</p> <p>A first - and major - problem is their current installation of OpenMPI. It does not use the proper libfabric library that provides good performance on LUMI. Apart from that, another issue might be compatibility with our Slurm installation, but that was not tested to write this review. The injection mechanism they are working on may or may not be a solution - depending also on the versions of MPI they want to install - but that development is not part of this development project, and it also requires access to a directory that is not managed by the application support team of LUMI but by sysadmins (though a symbolic link might provide a solution here). As the goal is to develop a workflow for CI/CD to evaluate performance and scalability, that is of no use as the  answer is known: bad performance and bad scalability for many codes without proper MPI support for the Slingshot interconnect. Prior development of a proper OpenMPI configuration would be needed first, but is not part of the project.</p> <p>Neither EESSI nor EasyBuild, that is used to install software in EESSI, currently support ROCm, nor is the development of that support part of the project. Hence we can only conclude that  the proposed software is not suitable for LUMI-G. Moreover, for the development of the actual CI/CD pipeline itself, it doesn't matter if it is done on LUMI-C or LUMI-G as both offer the same environment apart from the GPUs. Hence there is no need for an allocation on LUMI-G.</p> <p>Combining the above two elements: Until the next system update, it is also impossible to build an OpenMPI configuration with proper support  for GPU-aware MPI. After that update, the libfabric library and underlying Slingshot software should be OK to install the OpenMPI 5 branch with ROCm support, but it is not clear yet if the integration with Slurm  would work.</p> <p>EESSI developers and users should be aware that some software in the compatibility layer conflicts with some system tools.  The same may or may not hold for software in the EESSI stack itself. As long as only software in the compatibility layer and EESSI stack is used, this should not cause any problem. However, using software from the main system directories may not always work and produce warnings or crashes. We have seen issues in the past with EasyBuild-installed software on SUSE 15 SP4.  Though we were not able to try out a recent version of EESSI on a configuration equivalent to the current LUMI setup, we did note issues on a SUSE 15 SP5 setup that should be similar to the setup LUMI will offer after the August 2024 system update, though on that version of the OS, they did not lead to crashes.</p> <p>EESSI developers should be aware that the compute nodes on LUMI do not offer all daemons running in the background that one would expect on a regular Linux system. This may or may not have consequences for software installed in EESSI. We have run into problems running regular EasyBuild-installed packages on LUMI. As the compatibility layer contains libraries that may be different from those on LUMI, but not the daemons that may be needed, this layer does not solve all potential compatibility problems and more software than expected may need LUMI-specific configurations rather than the generic EESSI build.</p>"},{"location":"99_Misc/99_04_EESSI/#how-relevant-is-eessi-to-the-lumi-user","title":"How relevant is EESSI to the LUMI user?","text":""},{"location":"99_Misc/99_04_EESSI/#do-large-software-stacks-still-make-sense","title":"Do large software stacks still make sense?","text":"<p>Python users have been using virtual environments for a long time already as for many it is not possible to create a single environment that contains all packages they need due to version conflicts. </p> <p>Obviously any central Python installation will suffer from the same problem. Modules can help to deal with it, but it could become very confusing to see which modules work with which other modules if a Python installation is not done via a single Python packages module, but split across tens if not more of modules, and EESSI, being based on EasyBuild, follows the latter approach.</p> <p>However, the problems go further than Python packages. Researches use more an more poorly maintained codes, often remainders from another Ph.D. that are no longer maintained when the Ph.D. is finished and the author moves on to a totally  different job. Even some major packages don't always seem to  function properly with the latest version of a library. As a result it becomes impossible to rely on a single version of a package in a given toolchain, and long discussions about which versions should be included and for which versions users should use an older toolchain, are really slowing down development of EasyBuild and hence EESSI.</p> <p>This is currently already very visible as new toolchains often can only support CUDA at a much later date because the version of GCC on which the toolchain is built, is not yet supported by CUDA.</p> <p>Apart from that, large flat lists of modules are not really appreciated by users looking for their package, even though proper use of Lmod search tools can help a lot.</p> <p>At LUMI, we often see that </p> <ol> <li> <p>users have workflows that require specific versions of packages to be in a single toolchain so that they      can be loaded together, and</p> </li> <li> <p>more frequently actually, that users have a need for software packages with specific patches applied to them, and</p> </li> <li> <p>several packages that require licenses are in heavy use and these cannot be offered in EESSI.</p> </li> </ol> <p>Hence there are a lot of problems that EESSI does not solve at all. It would still require heavy building on top of EESSI, which currently still lacks a mechanism to do that in a way that integrates as nicely as with already installed software as we have on LUMI. Moreover, building on top of a software stack that you don't control, is harder than building on top of a software stack that you have built yourself.</p> <p>As EESSI uses standard EasyConfigs but plenty of hooks to modify them, one should be very aware of where EasyBuild stores what version of the EasyConfig (the edited one which actually mirrors what is actually installed, is stored in the <code>easybuild/reprod</code>  subdirectory of each package installation) or check multiple files to understand what an installation actually includes and does not include. Checking the log file is not that easy as it is compressed, limiting the tools that can be used to view that file without first uncompressing in a separate directory (not everybody is a fan of using <code>bzcat</code> to pipe through <code>less</code> and then use the <code>less</code>  navigation commands to search in that file).</p>"},{"location":"99_Misc/99_04_EESSI/#lack-of-support-for-amd-gpus","title":"Lack of support for AMD GPUs","text":"<p>At the time of writing of this paragraph (June 2024), CUDA support in EESSI is still in its infancy.</p> <p>GPU support in a central, multi-system software stack is even harder than offering optimal binaries for CPUS, as there are now basically two parameters to take into account. </p> <p>In the case of NVIDIA, you have to provide binaries built for various \"Compute Capabilities\", but also take into account that depending on the version of the driver found on the system, certain versions of CUDA may not function, disabling GPU functionality in certain toolchains or offering multiple CUDA versions in a single toolchain to offer the full range of systems.</p> <p>The problems are currently worse on AMD GPUs. There you have to take into account the series of the GPU for the proper instruction set and optimisations, a bit as for CPUs, but moreover, each driver version has basically only  support for 5 minor versions of ROCm. ROCm is still evolving rather quickly that certainly support for much newer ROCm versions than the one from the driver is not functional. It is not clear how well older versions are supported. The official support is very limited, but more versions may function. We do not yet have enough experience on LUMI with that to offer an answer to this question. It also means that one may even be in a situation when every  toolchain generation in EESSI supports only a single version of the ROCm libraries, that none of the toolchains may be guaranteed to function correctly on AMD hardware, and it is more than likely that only one will given that at the release rate of new ROCm versions the support windows seems to be smaller than one year of releases. An additional complication may even be that depending on how the CPU and GPU are linked, different settings at compile time may be beneficial for performance, leading, e.g., to different builds for MI250X on one side and  MI210/MI250 on the other, or MI300A versus MI300X.</p> <p>Note also that as of June 2024, there is no ROCm support at all in the official releases of EasyBuild hence  also no ROCm support possible in EESSI which makes EESSI currently irrelevant for LUMI GPU users.</p>"},{"location":"99_Misc/99_04_EESSI/#conflicting-settings-etc","title":"Conflicting settings etc.","text":"<p>EESSI comes with its own LMOD environment that is initialised when running the EESSI initialisation script. That LMOD may have some settings different from the settings chosen on LUMI, so that LMOD may behave differently which could be confusing to LUMI users. Obviously it is always possible to build a variant of the initialisation script specifically for a site, but that leads to two other problems:</p> <ol> <li> <p>It may be impossible to enforce users to use that site-specific script (not clear though as it might be that     EESSI could offer such a thing through site hooks that are enforced if they want to).</p> </li> <li> <p>It would then be confusing to EESSI users from another site. And the behaviour may no longer align with      the EESSI documentation.</p> </li> </ol> <p>Currently this is a rather hypothetical problem though as currently the behaviour of LMOD aligns rather well with that of LUMI, albeit without the various modules that LUMI offers to customise the behaviour. Moreover, all versions of the toolchains are thrown together while on LUMI we at least distinguish between various releases of the HPE Cray PE, which in EESSI would mean separate stacks for the 2022b, 2023a and 2023b toolchains.</p> <p>We can also never exclude that software installed with EESSI conflicts with software already installed in the OS. As long as users use only software installed with EESSI, there is no problem, but when they call tools installed through the system, this may cause problems. It is also less than trivial to use the debugging and profiling tools that come with the system.</p>"},{"location":"99_Misc/99_04_EESSI/#management-issues","title":"Management issues","text":""},{"location":"99_Misc/99_04_EESSI/#who-in-the-lumi-organisation-can-manage-eessi","title":"Who in the LUMI organisation can manage EESSI?","text":"<p>EESSI is better matched with a model where sysadmins are also responsible for the applications and less with the LUMI model where there is a strict wall between sysadmins and application support, with each their own zone on the system that is managed differently.</p> <p>As explained before, supporting CernVM-FS does require a considerable investment from the sysadmin team. Moreover, EESSI also assumes that the staff integrating EESSI with the system have full control over <code>/opt/eessi</code>, though this might be solvable on LUMI by making this a symbolic link to a <code>/appl/local</code>  subdirectory.</p> <p>Another issue is that the rhythm of work is not dictated by CSC or LUMI planning, but by the EESSI planning. E.g., when they decide to make a new toolchain available, it is up to us to quickly build a proper MPI library that can be injected in that installation as for the foreseeable future it is not possible for them to include the necessary support for LUMI in their MPI build (as they simply start from specific versions of libfabric also where the open source version may not yet include CXI support, and in fact, in the current public release - current is June 2024 - this support is still incomplete according to the HPE CoE). An external organisation dictating when LUST or CSC sysadmins should do certain work is not acceptable.</p>"},{"location":"99_Misc/99_04_EESSI/#how-to-organise-user-support","title":"How to organise user support?","text":"<p>The LUST cannot properly provide support for software it does not install as it does not have sufficient control over the way it is installed, and as in fact it cannot even install directly in the stack without going through a lengthy procedure assuring that it also works on all other sites, effectively turning the already understaffed LUST in a support team for other centres also. Since the installation is not done with HPE-provided tools, we can also not expect much support from the HPE side to solve problems. </p> <p>On the other hand, the EESSI community may not understand LUMI sufficiently enough and does not have enough control over LUMI to deal with issues that are caused by the specific LUMI environment.</p> <p>The EESSI community also significantly underestimates the impact of the diversity in the HPC landscape and the combinatorial problem they are running into, with</p> <ol> <li> <p>Support for three different processor architectures (x86, ARM and RISC-V) with all their subarchitectures requiring     specific optimisations,</p> </li> <li> <p>Two or three GPU architectures, with different subarchitectures (NVIDIA Compute Capabilities, AMD CDNA generations and     RDNA versions if you also want to offer compute support on some consumer GPUs) and restrictions in which versions of      the userland (that could be offered through EESSI with some license restrictions) can run on which versions of the drivers     (of which you can have only a single on the system, and controlled by the sysadmins of that system),</p> </li> <li> <p>Different interconnect architectures that may be hard to support in a single MPI build, or may even be impossible to     support with the chosen MPI implementation,</p> </li> <li> <p>Different configuration of the compute nodes with certain daemons disabled or configured differently as expected,     including a Slurm installation that may not have support for certain features enabled that the Open MPI implementation     expects to be present,</p> </li> <li> <p>And novel accelerator architectures that we may expect in the near future, especially for, e.g., more power-efficient     support for inference in deep learning models.</p> </li> </ol> <p>Basically the project will have to find a middle ground between what they can support in a central installation, through a  number of popular combinations of the elements above, and what would be easier to support with a fully local software  build as too many changes are required (and may not even match with versions of software chosen by EESSI). It is impossible for EESSI to support everything, basically leading to a number of system requirements for compatibility with EESSI that are  currently not yet clearly formulated. </p> <p>The EESSI community has no clear support model yet. The only workable way for LUST would be the same model as used  for the local software stacks, where the owner of the stack is responsible for support.</p>"},{"location":"99_Misc/99_04_EESSI/#needed-level-of-trust","title":"Needed level of trust","text":"<p>With EESSI, you run binaries that are fetched from the internet and where the update policies are determined by another organisation. This requires an even higher level of thrust in how the stack is managed than is required for the current local software stacks also offered on LUMI. </p> <p>If users find something today but it is removed tomorrow without warning, then this is not a good thing support-wise. Binaries should also not change or disappear while a job is using them (CernVM-FS should take care of this?) as otherwise those jobs would crash.</p> <p>Getting binaries from the internet is always a risk. Getting source code is also not completely safe as someone may have tampered with it and introduced malicious code. It is a risk that is always present. Extra for EESSI is that actually you cannot even blame the user or any organisation that is part of  the LUMI consortium if EESSI is abused by someone to push in malicious code as the control is completely in hands of an external project.</p> <p>All this requires thrust in the way the EESSI organisation manages and secures the software stack. That thrust has been damaged recently though this will not be discussed openly in this publicly available document.</p>"},{"location":"99_Misc/99_04_EESSI/#conclusion","title":"Conclusion","text":"<p>EESSI has a few explicit system requirements - e.g., support for CernVM-FS and some kind of access to <code>/opt/eessi</code> -  but likely more implied requirements that are not yet discovered or properly stated, e.g., support for certain features in system daemons or Slurm, certain driver versions for GPUs or other hardware directly supported by EESSI, ... Currently LUMI is not compatible with those requirements. Basically, a cluster has to be procured with specific requirements and extra hardware (for the Squid caches) to be suitable for EESSI, and the importance if this is  not yet recognised enough by the EESSI community. LUMI is not built for EESSI, due to its interconnect that is not yet sufficiently supported in open source versions of libraries, due to its novel GPU architecture that is not yet properly supported, due to the fact that we don't have the additional hardware needed to run CernVM-FS,  and due to the fact that the way CernVM-FS is typically installed conflicts with the HPE Cray ideas to control OS jitter for maximum scalability of large applications.</p> <p>There are a lot of problems that EESSI does not yet solve. In its current incarnation, apart from the requirement for CernVM-FS which is already a breaking point, it solves very few problems that we currently have and creates a lot of new problems.  It gives many of the support problems that we already have with containers without the benefit that containers can give us.</p> <p>There is a user support problem to which the EESSI community does not yet have a clear answer. </p> <p>To us, having something that is easy to adapt to the specific requirements of LUMI, makes some sense but then a local installation is required as changes may be in unexpected parts of the stack, which on their turn may propagate to other packages that depend on it. But that is basically more a version of EasyBuild with some EasyConfigs adapted to LUMI, where one can wonder if the software abstraction layer of  EESSI is really needed or does more harm (in being an additional source of conflict with tools on the system that users may want to use) than it is of benefit in having fewer EasyConfigs to modify.</p> <p>The one other way of managing software that would really be of help to LUMI would be one where a user can create a very personalised environment, if needed for file system performance in a container, otherwise on the Lustre file system and in a way that the tools that we have for performance monitoring etc. can be used as much as possible and not another tool that pushes users in very strict combinations of software that can be run together without continuously reloading modules between calls to separate steps in the workflow, and where you can then load that whole personalised environment in a single command (and have multiple such environments if the need arises, just as users can have multiple Conda environments or multiple Python virtual environments).</p>"},{"location":"99_Misc/99_05_Energy_consumption/","title":"Energy consumption on LUMI","text":""},{"location":"99_Misc/99_05_Energy_consumption/#general-information","title":"General information","text":"<ul> <li> <p>System counters are in <code>/sys/cray/pm_counters</code> and more specifically,      <code>/sys/cray/pm_counters/energy</code>.</p> </li> <li> <p>CUG 2018 paper on power management</p> </li> <li> <p>From a message of Orian in the chat:</p> <p>It is possible to compute it using the energy field using the sacct command with, for example, the following output format: <code>-oJobID,Elapsed,TRESUsageInTot%100</code>.</p> <p>I assume the values are in joule. The user can then get the carbon footprint by converting the joule in kWh (1 joule is 2.7778E-7 kWh) then eqCO2 (24 gCO2eq/kWh for hydroelectricity)</p> <p>But before answering to the user it would be good to confirm that the energy reported by Slurm is correct.</p> <p>Answer from Emmanuel: According to Fredrik, it seems correct for single-node computations but rather weird for multi-node ones.</p> <p>Measurements at the job step level should be used though, as measurements for the <code>batch</code> step are only for the first node.</p> <p>Divide by \\(3.6\\cdot 10^6\\) to get energy consumption in \\(\\mathrm{kWh}\\) as  \\(1 \\mathrm{kWh} = 1000 \\mathrm{J}/\\mathrm{s} \\times 3600 \\mathrm{s} = 3600000 \\mathrm{J} = 3.6\\cdot 10^6 \\mathrm{J}\\).</p> </li> </ul>"},{"location":"99_Misc/99_05_Energy_consumption/#pmt-power-measurement-toolkit","title":"PMT - Power Measurement Toolkit","text":"<p>From the paper \"Stefano Corda, Bram Veenboer, and Emma Trolley. PMT: Power Measurement Toolkit. 2022IEEE/ACM International Workshop on HPC User Support Tools (HUST) DOI: 10.1109/HUST56722.2022.00011\" (Preprint arXiv:2210.03724) (alternative link to the paper in the IEEE Computer Society Digital Library).</p> <p>BibTeX reference:</p> <pre><code>@INPROCEEDINGS{10027520,\n  author={Corda, Stefano and Veenboer, Bram and Tolley, Emma},\n  booktitle={2022 IEEE/ACM International Workshop on HPC User Support Tools (HUST)},\n  title={PMT: Power Measurement Toolkit},\n  year={2022},\n  volume={},\n  number={},\n  pages={44-47},\n  doi={10.1109/HUST56722.2022.00011}\n}\n</code></pre> <p>GitLab repository of the code @ ASTRON with release tags</p> <p>Installing (instructions from Orian)</p> <pre><code>module load PrgEnv-gnu rocm\ngit clone --recursive https://git.astron.nl/RD/pmt.git\nmkdir pmt/build\ncd pmt/build\nsed -i 's/Cray::name/name/' ../cray/Cray.h\ncmake -DPMT_BUILD_CRAY=ON \\\n    -DPMT_BUILD_ROCM=ON \\\n    -DCMAKE_C_COMPILER=gcc \\\n    -DCMAKE_CXX_COMPILER=g++ \\\n    -DCMAKE_INSTALL_PREFIX=&lt;changeme&gt; ..\nmake install\n</code></pre>"},{"location":"99_Misc/99_06_Answers_to_user_complaints/","title":"Answers to frequent user complaints","text":""},{"location":"99_Misc/99_06_Answers_to_user_complaints/#you-should-have-warned-that-my-data-is-removed-90-days-after-project-termination","title":"You should have warned that my data is removed 90 days after project termination","text":"<ul> <li> <p>All users get an email the day before the project ends, sent from <code>idmadm@csc.fi</code>,     with subject: \"Warning:LUMI project 45YXXXXXX end-date is approaching\" </p> <pre><code>Dear LUMI User,\n\nThis message is sent to all members of the project 46YXXXXXX (&lt;NAME OF THE PROJECT&gt;).\n\nYour LUMI project 46YXXXXXX is ending at &lt;DATE&gt;&gt;.\n\nThe end of the project will lead to termination of all resources (allocation) in the project. \nYou will be able to login and access your data. \nThe project data will be stored and available in the LUMI cluster for 90 days after the project\nlosure. After 90 days any data left in the LUMI cluster will be deleted. It is recommended to \ntransfer and remove the data from LUMI before the end of the 90 days grace period.\nIf you are the Project Principal Investigator (PI), please assure that all the project members \nare aware about the approaching end date. Project PI can request a project extension. Please \ncontact your national Resource Allocator for the extension request.\n</code></pre> <p>(There is a second mail when the project is actually fully closed but that doesn't help.)</p> </li> <li> <p>Agreed that we do not explicitly warn for the 90 days after project end date policy in the     documentation, but the      tables on the \"Data storage options\" page     do clearly say that there is no backup which already makes it clear that users should maintain     their own backup of precious data outside LUMI.</p> </li> <li> <p>We do warn that LUMI is not a long-term data storage option in our introductory courses.</p> </li> <li> <p>The fact that data use is billed should also make clear that that data will not stay forever     on the system when the project that can be billed for the data use has ended.</p> </li> <li> <p>TODO: Add this to the rotating part of the message-of-the-day.</p> </li> </ul>"}]}