{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Information that is missing or hard to find in the Cray PE manual or LUMI docs","text":"<p>This unofficial site contains things I discovered in the HPE-Cray PE or other software on LUMI but have trouble finding in the documentation or is too specialised or anecdotal to have a place in the documentation.</p> <p>Documentation links page maintained for the course pages</p> <ol> <li>Compilers<ol> <li>Documentation for the compilers</li> <li>Miscellaneous topics</li> </ol> </li> <li>LMOD<ol> <li>LMOD documentation</li> <li>Lmod setup</li> <li>PE module structure</li> </ol> </li> <li>Slurm<ol> <li>Doc links</li> <li>Miscellaneous tips&amp;tricks</li> </ol> </li> <li>AI software</li> <li>MPI<ol> <li>Cray MPICH tips&amp;tricks</li> <li>Open MPI</li> <li>OSU micro-benchmarks</li> <li>Misc</li> </ol> </li> <li>Lustre<ol> <li>Miscellaneous</li> </ol> </li> <li>ROCm<ol> <li>Miscellaneous</li> </ol> </li> <li>Linux<ol> <li>File management</li> </ol> </li> </ol>"},{"location":"00_00_introduction/","title":"Introduction","text":"<p>This unofficial site contains things I discovered in the HPE-Cray PE but have trouble finding in the documentation (probably also because the documentation is fairly hard to find on the web).</p>"},{"location":"Doclinks/","title":"Documentation links","text":"<p>Note that documentation, and especially web based documentation, is very fluid. Links change rapidly and were correct when this page was developed right after the course. However, there is no guarantee that they are still correct when you read this and will only be updated at the next course on the pages of that course.</p> <p>This documentation page is far from complete but bundles a lot of links mentioned during the presentations, and some more.</p>"},{"location":"Doclinks/#web-documentation","title":"Web documentation","text":"<ul> <li> <p>HPE Cray Programming Environment web documentation has only become available in      May 2023 and is a work-in-progress. It does contain a lot of HTML-processed man pages in an easier-to-browse      format than the man pages on the system.</p> <p>The presentations on debugging and profiling tools referred a lot to pages that can be found on this web site.  The manual pages mentioned in those presentations are also in the web documentation and are the easiest way  to access that documentation.</p> </li> <li> <p>Cray PE Github account with whitepapers and some documentation.</p> </li> <li> <p>Cray DSMML - Distributed Symmetric Memory Management Library</p> </li> <li> <p>Cray Library previously provides as TPSL build instructions</p> </li> <li> <p>Clang latest version documentation (Usually for the latest version)</p> <ul> <li> <p>Clang 13.0.0 version (basis for aocc/3.2.0)</p> </li> <li> <p>Clang 14.0.0 version (basis for rocm/5.2.3 and amd/5.2.3)</p> </li> <li> <p>Clang 15.0.0 version (cce/15.0.0 and cce/15.0.1 in 22.12/23.03)</p> </li> <li> <p>Clang 16.0.0 version (cce/16.0.0 in 23.09)</p> </li> </ul> </li> <li> <p>AMD Developer Information. Note that AMD doesn't archive     manuals of older versions which can be a problem. You have to reprocess them from GitHub repositories.</p> <ul> <li> <p>AOCC 4.0 CompilerOptions Quick Reference Guide      (Version 4.0 compilers will come when the 23.05 or later CPE release gets installed on LUMI and the system      is updated to COS 2.5 as some libraries are missing in COS 2.4)</p> </li> <li> <p>AOCC 4.0 User Guide</p> </li> </ul> </li> <li> <p>ROCm<sup>TM</sup> documentation overview</p> <ul> <li> <p>rocminfo application for reporting system info.</p> </li> <li> <p>rocm-smi</p> </li> <li> <p>HIP porting guide</p> </li> <li> <p>ROCm Software Platform GitHub repository</p> </li> <li> <p>Libraries:</p> <ul> <li> <p>BLAS: rocBLAS and hipBLAS</p> </li> <li> <p>FFTs: rocFFT and hipFFT</p> </li> <li> <p>Random number generation: rocRAND</p> </li> <li> <p>Sparse linear algebra: rocSPARSE and hipSPARSE</p> </li> <li> <p>Iterative solvers: rocALUTION</p> </li> <li> <p>Parallel primitives: rocPRIM and hipCUB</p> </li> <li> <p>Machine Learning Libraries: MIOpen (similar to cuDNN),      Tensile (GEMM Autotuner),     RCCL (ROCm analogue of NCCL) and      Horovod (Distributed ML)</p> </li> <li> <p>Machine Learning Frameworks: Tensorflow,     Pytorch and     Caffe</p> </li> <li> <p>Machine Learning Benchmarks:     DeepBench and      MLPerf</p> </li> </ul> </li> <li> <p>Development tools:</p> <ul> <li> <p>rocgdb resources:</p> <ul> <li> <p>AMD documentation</p> </li> <li> <p>2021 presentation by Justin Chang</p> </li> <li> <p>2021 Linux Plumbers Conference presentation     with youTube video with a part of the presentation</p> </li> </ul> </li> <li> <p>rocprof profiler</p> </li> <li> <p>OmniTrace</p> </li> <li> <p>Omniperf</p> </li> </ul> </li> </ul> </li> <li> <p>HDF5 generic documentation</p> </li> <li> <p>Mentioned in the Lustre presentation: The      ExaIO project paper     \"Transparent Asynchronous Parallel I/O Using Background Threads\".</p> </li> </ul> AMD documentation <p>AMD doesn't archive documentation for past versions of ROCM and their CPU compilers in a way that is ready-to-read. Instead </p>"},{"location":"Doclinks/#man-pages","title":"Man pages","text":"<p>A selection of man pages explicitly mentioned during the course:</p> <ul> <li> <p>Compilers</p> PrgEnv C C++ Fortran PrgEnv-cray <code>man craycc</code> <code>man crayCC</code> <code>man crayftn</code> PrgEnv-gnu <code>man gcc</code> <code>man g++</code> <code>man gfortran</code> PrgEnv-aocc/PrgEnv-amd - - - Compiler wrappers <code>man cc</code> <code>man CC</code> <code>man ftn</code> </li> <li> <p>OpenMP in CCE</p> <ul> <li><code>man intro_openmp</code></li> </ul> </li> <li> <p>OpenACC in CCE</p> <ul> <li><code>man intro_openacc</code></li> </ul> </li> <li> <p>MPI:</p> <ul> <li> <p>MPI itself: <code>man intro_mpi</code> or <code>man mpi</code></p> </li> <li> <p>libfabric: <code>man fabric</code></p> </li> <li> <p>CXI: `man fi_cxi'</p> </li> </ul> </li> <li> <p>LibSci</p> <ul> <li> <p><code>man intro_libsci</code> and <code>man intro_libsci_acc</code></p> </li> <li> <p><code>man intro_blas1</code>,     <code>man intro_blas2</code>,     <code>man intro_blas3</code>,     <code>man intro_cblas</code></p> </li> <li> <p><code>man intro_lapack</code></p> </li> <li> <p><code>man intro_scalapack</code> and <code>man intro_blacs</code></p> </li> <li> <p><code>man intro_irt</code></p> </li> <li> <p><code>man intro_fftw3</code></p> </li> </ul> </li> <li> <p>DSMML - Distributed Symmetric Memory Management Library </p> <ul> <li><code>man intro_dsmml</code></li> </ul> </li> <li> <p>Slurm manual pages are also all on the web      and are easily found by Google, but are usually those for the latest version.</p> <ul> <li> <p><code>man sbatch</code></p> </li> <li> <p><code>man srun</code></p> </li> <li> <p><code>man salloc</code></p> </li> <li> <p><code>man squeue</code></p> </li> <li> <p><code>man scancel</code></p> </li> <li> <p><code>man sinfo</code></p> </li> <li> <p><code>man sstat</code></p> </li> <li> <p><code>man sacct</code></p> </li> <li> <p><code>man scontrol</code></p> </li> </ul> </li> </ul>"},{"location":"Doclinks/#via-the-module-system","title":"Via the module system","text":"<p>Most HPE Cray PE modules contain links to further documentation. Try <code>module help cce</code> etc.</p>"},{"location":"Doclinks/#from-the-commands-themselves","title":"From the commands themselves","text":"PrgEnv C C++ Fortran PrgEnv-cray <code>craycc --help</code> <code>crayCC --help</code> <code>crayftn --help</code> <code>craycc --craype-help</code> <code>crayCC --craype-help</code> <code>crayftn --craype-help</code> PrgEnv-gnu <code>gcc --help</code> <code>g++ --help</code> <code>gfortran --help</code> PrgEnv-aocc <code>clang --help</code> <code>clang++ --help</code> <code>flang --help</code> PrgEnv-amd <code>amdclang --help</code> <code>amdclang++ --help</code> <code>amdflang --help</code> Compiler wrappers <code>cc --help</code> <code>CC --help</code> <code>ftn --help</code> <p>For the PrgEnv-gnu compiler, the <code>--help</code> option only shows a little bit of help information, but mentions further options to get help about specific topics.</p> <p>Further commands that provide extensive help on the command line:</p> <ul> <li><code>rocm-smi --help</code>, even on the login nodes.</li> </ul>"},{"location":"Doclinks/#documentation-of-other-cray-ex-systems","title":"Documentation of other Cray EX systems","text":"<p>Note that these systems may be configured differently, and this especially applies to the scheduler. So not all documentations of those systems applies to LUMI. Yet these web sites do contain a lot of useful information.</p> <ul> <li> <p>Archer2 documentation.      Archer2 is the national supercomputer of the UK, operated by EPCC. It is an AMD CPU-only cluster.     Two important differences with LUMI are that (a) the cluster uses AMD Rome CPUs with groups of 4 instead     of 8 cores sharing L3 cache and (b) the cluster uses Slingshot 10 instead of Slinshot 11 which has its     own bugs and workarounds.</p> <p>It includes a page on cray-python referred to during the course.</p> </li> <li> <p>ORNL Frontier User Guide and      ORNL Crusher Qucik-Start Guide.     Frontier is the first USA exascale cluster and is built up of nodes that are very similar to the     LUMI-G nodes (same CPA and GPUs but a different storage configuration) while Crusher is the     192-node early access system for Frontier. One important difference is the configuration of     the scheduler which has 1 core reserved in each CCD to have a more regular structure than LUMI.</p> </li> <li> <p>KTH Dardel documentation. Dardel is the Swedish \"baby-LUMI\" system.     Its CPU nodes use the AMD Rome CPU instead of AMD Milan, but its GPU nodes are the same as in LUMI.</p> </li> <li> <p>Setonix User Guide.     Setonix is a Cray EX system at Pawsey Supercomputing Centre in Australia. The CPU and GPU compute     nodes are the same as on LUMI.</p> </li> </ul>"},{"location":"01_Compilers/","title":"Compilers","text":"<ul> <li>Documentation for the compilers</li> <li>Miscellaneous topics</li> </ul>"},{"location":"01_Compilers/01_01_Doclinks/","title":"Documentation for the compilers","text":""},{"location":"01_Compilers/01_99_misc/","title":"Miscellaneous topics","text":""},{"location":"01_Compilers/01_99_misc/#using-the-compilers-without-the-wrappers","title":"Using the compilers without the wrappers","text":"Compiler module C compiler C++ compiler Fortran fixed format Fortran free format cce <code>craycc</code>, <code>clang</code> <code>crayCC</code>, <code>craycxx</code>, <code>clang++</code> <code>crayftn</code> <code>crayftn</code> gcc <code>gcc</code> <code>g++</code> <code>gfortran</code> <code>gfortran</code> aocc <code>clang</code> <code>clang++</code> <code>flang</code> <code>flang</code> rocm <code>amdclang</code> <code>amdclang++</code> <code>amdflang</code> <code>amdflang</code> <p>Remarks</p> <ul> <li> <p>The Cray clang compilers need to be pointed to the version of gcc to use using the     <code>--gcc-toolchain=&lt;value&gt;</code> option. The wrapper adds something like     <code>--gcc-toolchain=/opt/cray/pe/gcc/8.1.0/snos</code>.     Otherwise the compile will fail to find a suitable set of include files.</p> <p>TODO: Test this.</p> </li> <li> <p>A good trick to figure out which options the PE wrappers add is to use the     <code>-craype-verbose</code> flag on the command line of the wrappers.</p> </li> </ul>"},{"location":"02_LMOD/","title":"LMOD","text":"<ul> <li>Documentation links</li> <li>Lmod setup</li> <li>PE module structure</li> </ul>"},{"location":"02_LMOD/02_01_Doclinks/","title":"Documentation for LMOD","text":""},{"location":"02_LMOD/02_02_LMOD_setup/","title":"Cray Lmod setup","text":""},{"location":"02_LMOD/02_02_LMOD_setup/#2-versions-of-lmod","title":"2 versions of LMOD","text":"<p>Since the October-November 2023 update there are two versions of LMOD on the system</p> <ol> <li> <p>A version installed in the system directories, likely one that comes with SUSE linux.</p> <ul> <li>In November 2023 this is LMOD 8.3.1.</li> <li>Installation directory: <code>/usr/share/lmod/</code></li> </ul> </li> <li> <p>A version installed with the PE. </p> <ul> <li>In November 2023 this is LMOD 8.7.19 which comes with the 23.09 PE.</li> <li>Installation directory: <code>/opt/cray/pe/lmod</code></li> </ul> </li> </ol> <p>The active one after login is the one the comes with the HPE Cray PE, so when re-initialising be sure to call the initialisation functions of that version of LMOD.</p>"},{"location":"02_LMOD/02_02_LMOD_setup/#problems","title":"Problems","text":"<ul> <li> <p><code>module spider</code> does not work correctly. It does not always show all possible      combinations to access a module. This is because the HPE Cray PE doesn't use     hierarchies as intended by the LMOD developer. Sometimes a combination of two other     modules loaded in any order triggers a change to MODULEPATH.</p> </li> <li> <p>Noise about problems with the cache. We do note occasional cache problems on LUMI     but those don't seem to be associated with specific properties of the HPE Cray PE     Lmod setup but rather with the system not detecting that modules have been added     to the tree. I guess Linux with Lustre may have no notification system to detect     the moment of change in a subdirectory to see if a cache file is still valid and     to ignore (system cache) or regenerate (user cache) it otherwise.</p> </li> </ul>"},{"location":"02_LMOD/02_03_PE_module_structure/","title":"Custom paths","text":"<p>The information on this page is partly the result of studying <code>/opt/cray/pe/lmod_scripts/default/scripts/lmodHierarchy.lua</code> and other  module code.</p> <p>The HPE Cray PE can load custom paths with system-installed or user-installed  modules in a way that fits in the hierarchy defined by the PE. The following options are available:</p> <ul> <li> <p>Modules that depend on the CPU target module only.</p> <p>In the PE, these are in the <code>cpu</code> subdirectory and used for the <code>cray-fftw</code> modules.</p> <p>As these modules require loading only one module to be made available, there is no <code>handshake_</code> routine involved in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the network target module only, so not on the compiler or the     CPU architecture.</p> <p>In the PE, these are in the <code>net</code> subdirectory and used for the <code>cray-openshmemx</code> modules.</p> <p>As these modules require loading only one module to be made available, there is no <code>handshake_</code> routine involved in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the compiler only (so for a generic CPU)</p> <p>In the PE, these are in the <code>compiler</code> subdirectory and used for the <code>cray-hdf5</code>  modules.</p> <p>As these modules require loading only one module to be made available, there is no <code>handshake_</code> routine involved in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the CPU target module and the compiler.</p> <p>There are currently no such examples in the PE. however, the code is prepared for  future addition of such directories in the PE itself also and the reserved directory name is <code>comcpu</code>. </p> <p>As these modules require loading of two modules to be made available which in the PE can happen in any order as the hierarchy is not build in the typical Lmod way,  adding of both the PE and custom paths to <code>MODULEPATH</code> is managed by the <code>lmodHierarchy.handshake_comcpu</code> routine in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the network target module and the compiler.</p> <p>In the PE, these are in the <code>comnet</code> subdirectory and used for the <code>cray-mpich</code>, <code>cray-mpich-abi</code>, 'cray-mpich-ucx<code>and</code>cray-mpich-ucx-abi` modules.</p> <p>As these modules require loading of two modules to be made available which in the PE can happen in any order as the hierarchy is not build in the typical Lmod way,  adding of both the PE and custom paths to <code>MODULEPATH</code> is managed by the <code>lmodHierarchy.handshake_comnet</code> routine in <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on the network target module, the compiler and the MPI library.</p> <p>In the Cray PE, these are in the <code>mpi</code> subdirectory and used for the <code>cray-hdf5-parallel</code> and <code>cray-parallel-netcdf</code> modules.</p> <p>The addicion of the <code>mpi</code> subdiredctories is done in a way that is different from the other ones that depend on multiple modules, likely because the <code>cray-mpich-*</code> modules that add this level to the hierarchy themselves are at the <code>comnet</code> level and changed as soon as the network target or compiler module is changed, so that the regular Lmod approach of building a hierarchy can be used. The code that adds both the PE <code>mpi</code> directory and the corresponding custom path to the <code>MODULEPATH</code> is largely contained in  the <code>cray-mpich-*</code> modules but does use the <code>lmodHierarchy.get_user_custom_path</code> routine from <code>lmodHierarchy.lua</code>.</p> </li> <li> <p>Modules that depend on both the network and CPU target modules, on the compiler and      on the MPI library.</p> <p>There are currently no such examples in the PE. however, the code is prepared for  future addition of such directories in the PE itself also and the reserved directory name is <code>cncm</code>. </p> <p>As these modules require loading of two modules to be made available which in the PE can happen in any order as the hierarchy is not build in the typical Lmod way,  adding of both the PE and custom paths to <code>MODULEPATH</code> is managed by the <code>lmodHierarchy.handshake_cncm</code> routine in <code>lmodHierarchy.lua</code>.</p> </li> </ul> <pre><code>module_root\n\u251c\u2500 cpu\n\u2502  \u251c\u2500 x86-64\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u251c\u2500 x86-milan\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u251c\u2500 x86-rome\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u2514\u2500 x86-trento\n\u2502     \u2514\u2500 1.0\n\u251c\u2500 net\n\u2502  \u251c\u2500 ofi\n\u2502  \u2502  \u2514\u2500 1.0\n\u2502  \u2514\u2500 ucx\n\u2502     \u2514\u2500 1.0\n\u251c\u2500 compiler\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u251c\u2500 comcpu\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2502     \u251c\u2500 x86-64\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u251c\u2500 x86-milan\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u251c\u2500 x86-rome\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u2514\u2500 x86-trento\n\u2502  \u2502        \u2514\u2500 1.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u251c\u2500 comnet\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2502     \u251c\u2500 ofi\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u2514\u2500 ucx\n\u2502  \u2502        \u2514\u2500 1.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u251c\u2500 mpi\n\u2502  \u251c\u2500 aocc\n\u2502  \u2502  \u251c\u2500 2.2\n\u2502  \u2502  \u2514\u2500 3.0\n\u2502  \u251c\u2500 crayclang\n\u2502  \u2502  \u2514\u2500 10.0\n\u2502  \u2502     \u251c\u2500 ofi\n\u2502  \u2502     \u2502  \u2514\u2500 1.0\n\u2502  \u2502     \u2502     \u2514\u2500 cray-mpich\n\u2502  \u2502     \u2502        \u2514\u2500 8.0\n\u2502  \u2502     \u2514\u2500 ucx\n\u2502  \u2502        \u2514\u2500 1.0\n\u2502  \u2514\u2500 gnu\n\u2502     \u2514\u2500 8.0\n\u2514\u2500 cncm\n   \u251c\u2500 aocc\n   \u2502  \u251c\u2500 2.2\n   \u2502  \u2514\u2500 3.0\n   \u251c\u2500 crayclang\n   \u2502  \u2514\u2500 10.0\n   \u2502     \u251c\u2500 ofi\n   \u2502     \u2502  \u2514\u2500 1.0\n   \u2502     \u2502     \u251c\u2500 x86-64\n   \u2502     \u2502     \u2502  \u2514\u2500 1.0\n   \u2502     \u2502     \u2502     \u2514\u2500 cray-mpich\n   \u2502     \u2502     \u2502        \u2514\u2500 8.0\n   \u22ee     \u22ee      \u22ee   \n   \u2502     \u2514\u2500 ucx\n   \u2502        \u2514\u2500 1.0\n   \u2514\u2500 gnu\n      \u2514\u2500 8.0\n</code></pre> <p>Given that the custom paths are not set by a single environment variable pointing to the roots of the custom installation directories and imposing the same directory structure as used by the PE, but instead by separate environment variables for every leaf in the picture below, the number of environment variables can explode.</p> <p>On LUMI, as of July 2022, we have:</p> <ul> <li>3 CPU target modules and the generic one if we want to support that also,     so 4 CPU target modules.</li> <li>1 network library (though one could argue that UCX can still be used on the      login, large memory and visualisation GPU nodes but these are not really meant     for heavy MPI work unless they are integrating with jobs on the regular compute      nodes)</li> <li>4 compiler ABIs, with one to be dropped soon (aocc/2.2, aocc/3.0, crayclang/10.0 and gnu/8.0)</li> <li>1 MPI ABI version (cray-mpich/8.0)</li> </ul> <p>This may result in:</p> <ul> <li>4 <code>LMOD_CUSTOM_CPU_*</code> environment variables</li> <li>1 <code>LMOD_CUSTOM_NET_*</code> environment variable (or 2 with UCX)</li> <li>4 <code>LMOD_CUSTOM_COMPILER_*</code> environment variables</li> <li>16 <code>LMOD_CUSTOM_COMCPU_*</code> environment variables</li> <li>4 <code>LMOD_CUSTOM_COMNET_*</code> environment variables (8 with UCX)</li> <li>4 <code>LMOD_CUSTOM_MPI_*</code> environment variables (8 with UCX)</li> <li>16 <code>LMOD_CUSTOM_CNCM_*</code> environment variables (24 with UCX as two CPU architectures don't make sense with UCX     on LUMI)</li> </ul> <p>The names of the environment variables are derived from the above directory structure, but</p> <ul> <li>All characters uppercase</li> <li><code>/</code>, <code>-</code>and <code>.</code> get substituted by an underscore character</li> <li>The resulting name is prefixed with <code>LMOD_CUSTOM_</code> and postfixed with <code>_PREFIX</code>.</li> </ul>"},{"location":"03_Slurm/","title":"Slurm on LUMI","text":"<ul> <li>Doc links</li> <li>Miscellaneous tips&amp;tricks</li> </ul>"},{"location":"03_Slurm/03_01_Doclinks/","title":"Slurm documentation links","text":"<p>Note: Check the version of Slurm after each LUMI update.</p> <p>The version after the October-November 2023 update is 22.05.10.</p>"},{"location":"03_Slurm/03_01_Doclinks/#web-documentation","title":"Web documentation","text":"<ul> <li> <p>Slurm version 22.05.10, on the system at the time of the course</p> </li> <li> <p>Slurm manual pages are also all on the web      and are easily found by Google, but are usually those for the latest version.</p> <ul> <li> <p><code>man sbatch</code></p> </li> <li> <p><code>man srun</code></p> </li> <li> <p><code>man salloc</code></p> </li> <li> <p><code>man squeue</code></p> </li> <li> <p><code>man scancel</code></p> </li> <li> <p><code>man sinfo</code></p> </li> <li> <p><code>man sstat</code></p> </li> <li> <p><code>man sacct</code></p> </li> <li> <p><code>man scontrol</code></p> </li> </ul> </li> </ul>"},{"location":"03_Slurm/03_99_misc/","title":"Miscellaneous Slurm tips&amp;trics","text":""},{"location":"03_Slurm/03_99_misc/#system-configuration","title":"System configuration","text":"<ul> <li> <p>Extensive information about the partitions:     <pre><code>scontrol show partition --all\n</code></pre>     With `--all`` to also see hidden partitions.</p> </li> <li> <p>Show topology:     <pre><code>scontrol show topology\n</code></pre></p> </li> <li> <p>Show extensive configuration info:     <pre><code>scontrol show config\n</code></pre></p> </li> <li> <p>Also interesting:     <pre><code>scontrol show assoc_mgr\n</code></pre></p> </li> </ul>"},{"location":"03_Slurm/03_99_misc/#user-or-project-related","title":"User- or project-related","text":"<ul> <li> <p>Show which partitions a project has access to:     <pre><code>sacctmgr show assoc where account=project_462000087\n</code></pre></p> </li> <li> <p>Show which partitions a user has access to:     <pre><code>sacctmgr show assoc format=account,Partition  -p Users=kurtlust\n</code></pre></p> </li> </ul>"},{"location":"03_Slurm/03_99_misc/#job-management","title":"Job management","text":"<ul> <li>History of slurm jobs:<ul> <li>Through <code>sacct</code> <pre><code>sacct -a\n</code></pre></li> <li>Through <code>slurm</code> <pre><code>slurm history 2hours -a \n</code></pre>     or from a particular user:     <pre><code>slurm history 1day -u &lt;username&gt;\n</code></pre></li> </ul> </li> </ul>"},{"location":"03_Slurm/03_99_misc/#misc","title":"Misc","text":"<ul> <li> <p>Attach to a running job rather than ssh into a node:     <pre><code>srun --pty --jobid &lt;jobid&gt; bash\n</code></pre>     or     <pre><code>srun --pty --jobid &lt;jobid&gt; --w &lt;nodename&gt; bash\n</code></pre></p> </li> <li> <p>Testing on LUMI-D:     <pre><code>srun -plumid -t30:00 --gres=gpu:1 --pty bash\n</code></pre></p> </li> </ul>"},{"location":"04_AI_packages/","title":"AI software on LUMI","text":""},{"location":"05_MPI/","title":"MPI","text":"<ul> <li> <p>Cray MPICH tips&amp;tricks</p> </li> <li> <p>Open MPI</p> </li> <li> <p>OSU micro-benchmarks</p> </li> <li> <p>Misc</p> </li> </ul>"},{"location":"05_MPI/05_01_Cray_MPICH_tips/","title":"Cray MPICH tips","text":""},{"location":"05_MPI/05_01_Cray_MPICH_tips/#remarks-from-the-november-2023-hackathon","title":"Remarks from the November 2023 hackathon","text":"<ul> <li> <p>Using hugepages can improve intra-node performance (or performance overall?)</p> </li> <li> <p>Intra-node performance can improve by setting</p> <pre><code>export MPICH_SMP_SINGLE_COPY_MODE=NONE\n</code></pre> <p>at least for large message sizes.</p> <p>(Partly) documented in the <code>intro_mpi</code> man page.</p> </li> </ul>"},{"location":"05_MPI/05_01_Cray_MPICH_tips/#ofi-error-messages-and-potential-workarounds","title":"OFI error messages and potential workarounds","text":"<ul> <li> <p><code>PLTE_NOT_FOUND</code></p> <p>Error message:</p> <pre><code>MPIDI_OFI_handle_cq_error(1062): OFI poll failed (ofi_events.c:1064:MPIDI_OFI_handle_cq_error:Input/output error - PTLTE_NOT_FOUND)\n</code></pre> <p>HPE Cray recommendation:</p> <pre><code>export FI_CXI_MSG_OFFLOAD=0\nexport FI_CXI_REQ_BUF_COUNT=200\n</code></pre> <p>These are undocumented environment variables.</p> </li> <li> <p><code>UNDELIVERABLE</code></p> <p>Error message:</p> <pre><code>MPIDI_OFI_handle_cq_error(1067): OFI poll failed (ofi_events.c:1069:MPIDI_OFI_handle_cq_error:Input/output error - UNDELIVERABLE)\n</code></pre> <p>One possible cause is a broken switch so that messages can no longer be delivered.</p> <ul> <li>Symptoms on the GPU nodes: A pattern of a number of subsequent either even-numbered or odd-numbered nodes.</li> </ul> </li> </ul>"},{"location":"05_MPI/05_01_Cray_MPICH_tips/#other-mpi-error-messages","title":"Other MPI error messages","text":"<ul> <li> <p><code>xpmem_attache failed on</code></p> <p>Error message:</p> <pre><code>MPIDI_CRAY_XPMEM_do_attach(482)...........: xpmem_attach failed on rank 5 (src_rank 20, vaddr 0x3abf5b20, len 2400)\n</code></pre> <p>It is a memory registration cache bug. The default MR monitor is known to be buggy. The memory registration cache keeps track of what memory regions have been pinned and mapped to the network interface. A bug in the registration cache monitor can have some unexpected side effects, like memory regions deallocated by the code might not be correctly unregistered, which can result in future buffers that share the same virtual addresses being mapped incorrectly or thrown back. Steve Abbott (HPE Frontier support) thinks memhooks is better but hasn't gotten much testing yet, and long term (towards the end of 2023) HPE intends to implement a solution that makes it irrelevant and is better all around.</p> <p>Solutions:</p> <ul> <li> <p>Preferred: </p> <pre><code>export FI_MR_CACHE_MONITOR=memhooks\n</code></pre> <p>to use the memhooks memory monitor.</p> <p>Undocumented (or has this become irrelevant in the current versions)?</p> </li> <li> <p>Alternative:</p> <pre><code>export FI_MR_CACHE_MAX_COUNT=0\n</code></pre> <p>which turns off the memory registration cache all together but mibht be pretty punishing on performance (code-dependent).</p> <p>Documented in the <code>intro_mpi</code> man page.</p> </li> </ul> </li> </ul>"},{"location":"05_MPI/05_01_Cray_MPICH_tips/#investigating-crashes","title":"Investigating crashes","text":"<p>Based on tips from Juha Jaykka.</p> <ul> <li> <p>Use <code>sacct</code> to figure out what nodes a job is using and in what time interval it ran:</p> <pre><code>sacct --job=$jobid --format=JobID,JobName,State,ExitCode,Start,End,NodeList%1000\n</code></pre> </li> <li> <p>Now use <code>sacctmgr</code> to list events on those nodes during the time window. You can      copy-paste date/times and the nodelist from the previous command:</p> <pre><code>sacctmgr list event Start=&lt;starttime&gt; End=&lt;endtimne&gt; Nodes=&lt;nodelist&gt; format=NodeName,TimeStart,Reason%150\n</code></pre> </li> <li> <p>Sometimes you can get more information about a specific node with a fault via <code>scontrol show node</code>.      So for a faulty node from the previous command, you can run:</p> <pre><code>scontrol show node &lt;nodename or nodelist&gt;\n</code></pre> </li> </ul> <p>Note that after many node failures, a node goes automatically into drain mode, so it does not make sense to manually exclude them in your job scripts.</p>"},{"location":"05_MPI/05_02_OpenMPI/","title":"Open MPI on LUMI","text":""},{"location":"05_MPI/05_02_OpenMPI/#build-instructions-alfio-from-the-hackathon","title":"Build instructions Alfio from the hackathon","text":"<pre><code>module load PrgEnv-gnu\nmodule unload cray-mpich\nOMPI_VERSION=4.1.6\necho \"Install OpenMPI v\"$OMPI_VERSION\nwget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-$OMPI_VERSION.tar.bz2\ntar xf openmpi-$OMPI_VERSION.tar.bz2 &amp;&amp; rm openmpi-$OMPI_VERSION.tar.bz2\ncd openmpi-$OMPI_VERSION\n\n# SLURM (default yes), OFI, PMI\nmake clean\n./configure --prefix=$PWD/install_ofi_slurm_pmi/ \\\n--with-ofi=/opt/cray/libfabric/1.15.2.0 --with-slurm --with-pmi=/usr CC=gcc CXX=g++ FTN=gfortran CFLAGS=\"-march=znver2\" \\\n--disable-shared --enable-static\nmake -j 10 install\n\nrm -f install\nln -s $PWD/install_ofi_slurm_pmi install\n\nmodule unload cray-mpich\nexport PATH=$PWD/install/bin/:$PATH\nexport LD_LIBRARY_PATH=$PWD/install/lib:$LD_LIBRARY_PATH\n\n# Test with OSU\nwget https://mvapich.cse.ohio-state.edu/download/mvapich/osu-micro-benchmarks-5.9.tar.gz\ntar xf osu-micro-benchmarks-5.9.tar.gz\nrm osu-micro-benchmarks-5.9.tar.gz\ncd osu-micro-benchmarks-5.9\n\n./configure --prefix=$PWD/install CC=$(which mpicc) CXX=$(which mpicxx) CFLAGS=-O3 CXXFLAGS=-O3\nmake -j 10 install\n\ncd install/libexec/osu-micro-benchmarks/mpi/p2p \nsrun -n 2 --ntasks-per-node=1 ./osu_bw\n\n# Cannot remember the performance, try to compare to cray-mpich\n</code></pre> <p>It looks like you can now safely build shared libraries though.</p> <p>Useful links:</p> <ul> <li>Use at NERSC</li> </ul>"},{"location":"05_MPI/05_02_OpenMPI/#open-mpi-5-for-slingshot-11","title":"Open MPI 5 for Slingshot 11","text":"<p>Useful links:</p> <ul> <li> <p>CUG paper ORNL</p> <p>The paper says that changes needed in Open MPI itself are present in the Open MPI main and v5.0.x branches, but not in branches for older versions.</p> <p>One problem with Open MPI on OFI+CXI was that the CXI provider has no special pathway for intra-node messaging. Rather than redeveloping the MTL to deal with multiple components concurrently, a new provider was made, LINKx, encapsulating CXI but adding the missing features.</p> </li> <li> <p>Use at NERSC</p> </li> </ul>"},{"location":"05_MPI/05_02_OpenMPI/#open-mpi-with-spack","title":"Open MPI with Spack","text":"<p>Spack needs to be told how to use the proper libfabric library.</p> <p>A user gave some information in ticket #3228 on how they did it:</p> <ul> <li> <p>Add to the spack.yaml file:</p> <pre><code>packages:\n  libfabric:\n    externals:\n      - spec: libfabric@1.15.2.0\n        prefix: /opt/cray/libfabric/1.15.2.0\n</code></pre> <p>I guess this could go in <code>/etc/spack/packages.yaml</code> in the Spack installation itself.</p> </li> <li> <p>When calling Spack to build Open MPI, specify <code>^libfabric@1.15.2.0</code>.</p> </li> </ul>"},{"location":"05_MPI/05_03_OSU_benchmarks/","title":"OSU micro-benchmarks","text":"<ul> <li> <p>Home page of the benchmarks</p> </li> <li> <p>Examples of running the benchmarks:</p> <ul> <li> <p>University of Luxemburg HPC tutorial page</p> </li> <li> <p>AWS HPC workshop</p> </li> <li> <p>Sigma2 page</p> </li> </ul> </li> </ul>"},{"location":"05_MPI/05_99_misc/","title":"Misceallaneous topics","text":""},{"location":"05_MPI/05_99_misc/#cxi-went-public","title":"CXI went public","text":"<p>At the end of January 2024, CXI was upstreamed to the  libfabric GitHub repository in the libfabric/prov/cxi subdirectory, so it is now possible to build libfabric ourselves (at least once the bugs are fixed, on February 9 2024 Alfio mentioned that there are still problems with missing header files etc.)</p>"},{"location":"06_Lustre/","title":"Lustre","text":"<ul> <li>Miscellaneous</li> </ul>"},{"location":"06_Lustre/06_99_misc/","title":"Miscellaneous topics","text":"<ul> <li>Tips for speeding up modules (from Jean-Yves Vet, HPE)<ul> <li>DNE2/3 on the directories containing the modules (I don't believe this is enabled as      most of the    metadata points to a single MDS).</li> <li>Write those small files as DoM (data over metadata) to avoid the OSS round trip</li> </ul> </li> </ul>"},{"location":"07_ROCm/","title":"ROCm","text":"<ul> <li>Miscellaneous</li> </ul>"},{"location":"07_ROCm/07_99_misc/","title":"ROCm miscellaneous issues","text":""},{"location":"07_ROCm/07_99_misc/#driver-version-compatibility","title":"Driver-version compatibility","text":""},{"location":"07_ROCm/07_99_misc/#523-driver-version","title":"5.2.3 driver version","text":"<ul> <li> <p>ROCm 5.4.3 appears to work</p> </li> <li> <p>ROCm 5.6.1: </p> <ul> <li>According to Samuel there are some known issues with queries     about available memory being off but it hasn't been a blocker for apps.</li> <li>Peter mentioned that he couldn't get Neko to work with ROCm 5.,6, with a failure in     <code>hipDeviceGetStreamPriorityRange</code> right at the start.</li> </ul> </li> </ul>"},{"location":"08_Software/","title":"Remarks about individual software packages","text":"<ul> <li>[GROMACS])08_00_GROMACS.md)</li> </ul>"},{"location":"08_Software/08_00_GROMACS/","title":"GROMACS","text":"<ul> <li>Interesting link about a <code>hipMemsetAsync()</code> error message.     The discussion has input from one of the developers.</li> </ul>"},{"location":"99_Linux/","title":"Rather more general Linux tips","text":"<ul> <li>File management</li> </ul>"},{"location":"99_Linux/99_01_File_management/","title":"File management tips and tricks","text":"<ul> <li> <p>How to quickly check the number of files?</p> <ul> <li>Solution with <code>rsync</code>:     <pre><code>rsync --stats --dry-run\n</code></pre></li> </ul> </li> </ul>"}]}